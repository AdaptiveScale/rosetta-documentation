{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enterprise Data Management, powered by Metadata! Data is a key asset for enterprises, yet finding the right data on the right systems can be elusive. AdaptiveScale allows you to leverage the metadata from your data sources to discover and organize data. AdaptiveScale helps catalog all the data sources and files that a user wants to discover and centralizes that metadata in a single location. Catalog generation with tagging and labeling is supported down to column level and the scheduling is customizable for catalog triggers. Scheduling is an important part oft he catalloging process because as data is mutated in respective data sources something needs to keep track of what changes are taking place and captures that lienage. All your metadate is searchable and it's powered by ElasticSearch. Users can also query and browse any JDBC compliant database in the SQL Explorer. You can also use your catalog as a curated source for transfering the data, with associated filtering and governance policies applied to cloud storage or Hadoop HDFS. Cloud storage support includes Amazon S3, Azure Data Lake Storage and Google Cloud Storage. Categories The dashboard provides general statistic for how many data sources the user has crawled, how many catalogs have been created, and how many tags or lables are created. The AdaptiveScale JDBC driver which you can download directly from the dashboard allows you to connect to AdaptiveScale using your favorite JDBC compliant development environment to browse and query cataloged data sources with SQL. To help the users get more familiar with AdaptiveScale, there are links to User Guides, Tutorials and Documentation. The user interface is organized in the following categories: Data Sources, Catalogs, Tags, Schedules, Schema Evolution, Search, SQL Explorer and Data Tranfer. Data sources Data Sources is the category where the user defines a data source connection. There are six options to choose from; a JDBC driver, file, file transfer protocol(FTP), Google Bucket, Amazon S3, and Microsoft Azure. The supported database servers are: MySQL, MS SQL Server, Postgres, Oracle, BigQuery, and Generic driver. The Generic driver expands the database list to any JDBC compliant database so you can connect to any database that provides a JDBC driver. To create a new data source connection fill in the connection name, description, a host and a port for the server, database name for the utilized database server with a username and password, define a driver class name and upload a JDBC driver. All these fields are saved and the newly created data source connection is shown in a list of Data Source connections. Once saved the connection will be displayed in a list where you have the option to edit or delete it. For a new file or FTP data source connection fill in the connection name, choose a URI of the file, and from the dropdown choose CSV or AVRO, which should be same as the type of the file. The same applies for the other connections that are left with some small differences, for example, for Google Bucket data source connection you must add the Service Account, for Amazon S3 you must provide the Access Key ID and Secret Access Key and for Microsoft Azure you should provide Account Name, Container Name and SAS Token. Catalogs Catalogs is where you create the catalog based on the metadata of the data source. In catalog generation, tagging and labeling are supported. To create a new catalog the user needs to fill the catalog name, which needs to be a unique name to prevent any kind of naming collisions, choose a data source from existing ones or the newly created from where all the columns, tables are going to be retrieved. In the Details section you have the option to fetch the total counts of records within the data source. Record counts are fetched every time a scheduled catalog is triggered and will automatically be included in the catalog metadata. There are two important features inside the catalog category, one is for tagging and labeling, and the other for scheduling, where you can select a time interval for when the catalog crawler shoudl be run. All these changes are saved and the new catalog is shown in a list of catalogs with fields that have all the statistics calculated and set in the previous step, including how many tables and columns were crawled, when it was last run, next run and options to edit, delete and run the catalog. With run option, the crawler retrieves all the information from the data source and it applies the catalog rules set. Business Rules Business Rules allow you to automatically tag tables and columns with patern matching. To create a business tag, the user needs to provide a tag name, description, and optionally a collection name. In tags section you can also apply matching patterns to tag more than one column with same characteristics and also apply Business Rules for data governance by masking and filtering data using JavaScript code blocks. You can also define your tags externally and import them as a CSV file. Schedules Schedules lists all active and inactive crawlers defined for catalogs, with information like the catalog name, last run, next scheduled run, and options to activate or deactivate a schedule, and the option to delete it. Schema evolution Schema evolution allows you to choose a catalog and see the changes that have been made to that catalog over time. The lineage can be seen very clearly; for example when the user selects a date, the column or table that has been changed is highlighted with highlight colors. Search Search schreen allows you to search by tags, field names, table names, data source names, and more. Here\u2019s a list of supported filters: string, wild card: *(string)* tag: <tag name (string)> key: <key name (string)> val: <(string)> table: <table name> column: <field name> data source: <data source name> If for example, you searche for a tag name, it retrieves that tag with how many matches it finds. The user can also see the data lineage from the search section when clicking on the column. To understand more about it, see the sections on the right. SQL Explorer SQL Explorer allows you to choose a JDBC data source connection, view the schema and query or browse any JDBC compliant database in the SQL. Data Transfer Data Transfer is used for exporting data from a selected catalog to cloud storage and HDFS, including Google Cloud Storage, Amazon S3, and Azure Data Lake Storage.","title":"Introduction"},{"location":"#categories","text":"The dashboard provides general statistic for how many data sources the user has crawled, how many catalogs have been created, and how many tags or lables are created. The AdaptiveScale JDBC driver which you can download directly from the dashboard allows you to connect to AdaptiveScale using your favorite JDBC compliant development environment to browse and query cataloged data sources with SQL. To help the users get more familiar with AdaptiveScale, there are links to User Guides, Tutorials and Documentation. The user interface is organized in the following categories: Data Sources, Catalogs, Tags, Schedules, Schema Evolution, Search, SQL Explorer and Data Tranfer.","title":"Categories"},{"location":"#data-sources","text":"Data Sources is the category where the user defines a data source connection. There are six options to choose from; a JDBC driver, file, file transfer protocol(FTP), Google Bucket, Amazon S3, and Microsoft Azure. The supported database servers are: MySQL, MS SQL Server, Postgres, Oracle, BigQuery, and Generic driver. The Generic driver expands the database list to any JDBC compliant database so you can connect to any database that provides a JDBC driver. To create a new data source connection fill in the connection name, description, a host and a port for the server, database name for the utilized database server with a username and password, define a driver class name and upload a JDBC driver. All these fields are saved and the newly created data source connection is shown in a list of Data Source connections. Once saved the connection will be displayed in a list where you have the option to edit or delete it. For a new file or FTP data source connection fill in the connection name, choose a URI of the file, and from the dropdown choose CSV or AVRO, which should be same as the type of the file. The same applies for the other connections that are left with some small differences, for example, for Google Bucket data source connection you must add the Service Account, for Amazon S3 you must provide the Access Key ID and Secret Access Key and for Microsoft Azure you should provide Account Name, Container Name and SAS Token.","title":" Data sources "},{"location":"#catalogs","text":"Catalogs is where you create the catalog based on the metadata of the data source. In catalog generation, tagging and labeling are supported. To create a new catalog the user needs to fill the catalog name, which needs to be a unique name to prevent any kind of naming collisions, choose a data source from existing ones or the newly created from where all the columns, tables are going to be retrieved. In the Details section you have the option to fetch the total counts of records within the data source. Record counts are fetched every time a scheduled catalog is triggered and will automatically be included in the catalog metadata. There are two important features inside the catalog category, one is for tagging and labeling, and the other for scheduling, where you can select a time interval for when the catalog crawler shoudl be run. All these changes are saved and the new catalog is shown in a list of catalogs with fields that have all the statistics calculated and set in the previous step, including how many tables and columns were crawled, when it was last run, next run and options to edit, delete and run the catalog. With run option, the crawler retrieves all the information from the data source and it applies the catalog rules set.","title":" Catalogs "},{"location":"#business-rules","text":"Business Rules allow you to automatically tag tables and columns with patern matching. To create a business tag, the user needs to provide a tag name, description, and optionally a collection name. In tags section you can also apply matching patterns to tag more than one column with same characteristics and also apply Business Rules for data governance by masking and filtering data using JavaScript code blocks. You can also define your tags externally and import them as a CSV file.","title":" Business Rules "},{"location":"#schedules","text":"Schedules lists all active and inactive crawlers defined for catalogs, with information like the catalog name, last run, next scheduled run, and options to activate or deactivate a schedule, and the option to delete it.","title":" Schedules "},{"location":"#schema-evolution","text":"Schema evolution allows you to choose a catalog and see the changes that have been made to that catalog over time. The lineage can be seen very clearly; for example when the user selects a date, the column or table that has been changed is highlighted with highlight colors.","title":" Schema evolution "},{"location":"#search","text":"Search schreen allows you to search by tags, field names, table names, data source names, and more. Here\u2019s a list of supported filters: string, wild card: *(string)* tag: <tag name (string)> key: <key name (string)> val: <(string)> table: <table name> column: <field name> data source: <data source name> If for example, you searche for a tag name, it retrieves that tag with how many matches it finds. The user can also see the data lineage from the search section when clicking on the column. To understand more about it, see the sections on the right.","title":" Search "},{"location":"#sql-explorer","text":"SQL Explorer allows you to choose a JDBC data source connection, view the schema and query or browse any JDBC compliant database in the SQL.","title":" SQL Explorer "},{"location":"#data-transfer","text":"Data Transfer is used for exporting data from a selected catalog to cloud storage and HDFS, including Google Cloud Storage, Amazon S3, and Azure Data Lake Storage.","title":" Data Transfer "},{"location":"adminpanel/","text":"Admin Panel is a feature that allows users to backup and restore the project settings, and also it is possible to create a git configuration so the changes made can be deployed there. On the right corner at the top there is an icon, when clicked it shows the Admin Panel. Admin Panel has two tabs, Backup and Git Config . Example Create a backup and restore To backup the project settings, the user must go on Admin Panel and choose Backup tab and click the Backup & Download button. This will create a zip file with all the settings which later on can be restored when needed by clicking on the Restore button and upload the same zip file. Add a git configuration Git configuration is needed if the user wants to deploy the collection changes made in the project. In order to use the git feature the user must create one by choosing on Git Config tab and then click on Add Git Config button. In Git Configuration the user must choose a type of the git such as Github, Gitlab, or Bitbucket, fill the name that should be unique, add the url, insert the branch name and provide the authentication key. Edit a git configuration Now that one git is configured, on Git Config tab the user can see a list of the git configurations and a button Add Git Config that redirects to the Git Configuration screen for creating a new configuration. The configuration can be edited and deleted. When the user clicks on the pen icon next to the configuration from the list they can edit the type, name, url, branch name or token, and save the changes. Delete a configuration The deletion of a configuration is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the configuration, and if yes the user clicks Delete and if not clicks Cancel.","title":"Admin Panel"},{"location":"adminpanel/#example","text":"","title":"Example"},{"location":"adminpanel/#create-a-backup-and-restore","text":"To backup the project settings, the user must go on Admin Panel and choose Backup tab and click the Backup & Download button. This will create a zip file with all the settings which later on can be restored when needed by clicking on the Restore button and upload the same zip file.","title":"Create a backup and restore"},{"location":"adminpanel/#add-a-git-configuration","text":"Git configuration is needed if the user wants to deploy the collection changes made in the project. In order to use the git feature the user must create one by choosing on Git Config tab and then click on Add Git Config button. In Git Configuration the user must choose a type of the git such as Github, Gitlab, or Bitbucket, fill the name that should be unique, add the url, insert the branch name and provide the authentication key.","title":"Add a git configuration"},{"location":"adminpanel/#edit-a-git-configuration","text":"Now that one git is configured, on Git Config tab the user can see a list of the git configurations and a button Add Git Config that redirects to the Git Configuration screen for creating a new configuration. The configuration can be edited and deleted. When the user clicks on the pen icon next to the configuration from the list they can edit the type, name, url, branch name or token, and save the changes.","title":"Edit a git configuration"},{"location":"adminpanel/#delete-a-configuration","text":"The deletion of a configuration is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the configuration, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete a configuration"},{"location":"alerts/","text":"Alerts category allows users to set up alerts for their catalogs. Setting up alerts in Adaptive Scale is straightforward by filling some general info such as name, description, then choose a catalog, an input rule and out rule. Example Create an alert To create an alert, the user simply needs to click on the Alerts tab from the menu on the left. If there are no alerts configured, the user clicks on New Alert button to add a new alert. After the button is clicked, the Alerts screen is opened. The user must fill the general information such as Alerts name, description and choose the catalog for which it needs to create the alert. The next step is to choose an input rule between Size in bytes , Total records and SQL checks , and then choose an execution rule between Run SQL , API Call and Send Email . Input rules Size in bytes - allows users to choose a table from the selected catalog and check a value with an operator. Total records - allows users to choose a table from the selected catalog and check the total records by choosing an operator and give a value for that table. SQL Check - The SQL input rule will be fulfilled if the query returns any row. Execution rules If the input rule is fullfiled then the execution rule gets executed. Run SQL - on the dropdown choose a data source, and the SQL query will be executed on that data source. Api Call - the user is able to make a GET and a POST api call, by giving the uri, and fill the header, value and the body if needed. Send Email - this allows users to send an email. After clicking Save button all the configurations are saved and the alert is ready. Edit an alert Now that an alert is configured, on Alerts tab the user can see a list of the alerts and a button Add Alert that redirects to the Alerts screen for creating a new one. The alert can be edited, deleted, activated/deactived and searched. When the user clicks on the pen icon next to the alert from the list they can edit the name, description, the catalog for the alert, also change the input and execution rules. Delete an alert The deletion of an alert is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the collection, and if yes the user clicks Delete and if not clicks Cancel. Search for an alert The user can search for an alert from the list of alerts. The alert that we created as an example is named Alert Demo , so if the user searches for test there will be no results. Activate/Deactivate alert When the user creates an alert, by default it is set in Active alerts list. This means that the next run of the catalog will trigger the alert, and it will be visible at the bell icon on the top. If the user clicks on the alert is able to see the alerts detailed report, the history and status of the run. All this happens if the toggle is on Active, if the toggle is set to Deactive than the user won't get an alert when it runs the catalog. The grey color indicates that the alert is deactive.","title":"Alerts"},{"location":"alerts/#example","text":"","title":"Example"},{"location":"alerts/#create-an-alert","text":"To create an alert, the user simply needs to click on the Alerts tab from the menu on the left. If there are no alerts configured, the user clicks on New Alert button to add a new alert. After the button is clicked, the Alerts screen is opened. The user must fill the general information such as Alerts name, description and choose the catalog for which it needs to create the alert. The next step is to choose an input rule between Size in bytes , Total records and SQL checks , and then choose an execution rule between Run SQL , API Call and Send Email .","title":"Create an alert"},{"location":"alerts/#input-rules","text":"Size in bytes - allows users to choose a table from the selected catalog and check a value with an operator. Total records - allows users to choose a table from the selected catalog and check the total records by choosing an operator and give a value for that table. SQL Check - The SQL input rule will be fulfilled if the query returns any row.","title":"Input rules"},{"location":"alerts/#execution-rules","text":"If the input rule is fullfiled then the execution rule gets executed. Run SQL - on the dropdown choose a data source, and the SQL query will be executed on that data source. Api Call - the user is able to make a GET and a POST api call, by giving the uri, and fill the header, value and the body if needed. Send Email - this allows users to send an email. After clicking Save button all the configurations are saved and the alert is ready.","title":"Execution rules"},{"location":"alerts/#edit-an-alert","text":"Now that an alert is configured, on Alerts tab the user can see a list of the alerts and a button Add Alert that redirects to the Alerts screen for creating a new one. The alert can be edited, deleted, activated/deactived and searched. When the user clicks on the pen icon next to the alert from the list they can edit the name, description, the catalog for the alert, also change the input and execution rules.","title":"Edit an alert"},{"location":"alerts/#delete-an-alert","text":"The deletion of an alert is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the collection, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete an alert"},{"location":"alerts/#search-for-an-alert","text":"The user can search for an alert from the list of alerts. The alert that we created as an example is named Alert Demo , so if the user searches for test there will be no results.","title":"Search for an alert"},{"location":"alerts/#activatedeactivate-alert","text":"When the user creates an alert, by default it is set in Active alerts list. This means that the next run of the catalog will trigger the alert, and it will be visible at the bell icon on the top. If the user clicks on the alert is able to see the alerts detailed report, the history and status of the run. All this happens if the toggle is on Active, if the toggle is set to Deactive than the user won't get an alert when it runs the catalog. The grey color indicates that the alert is deactive.","title":"Activate/Deactivate alert"},{"location":"catalogs/","text":"Catalog is the category where the user creates the catalog based on the metadata of the data source. In catalog generation, tagging and labeling are supported. To create a new catalog the user needs to fill the catalog name which needs to be a unique name to prevent any kind of issues that might happen, choose a data source from existing ones or the newly created from where all the columns, tables are going to be retrieved. There is also a part for lazy loading that fetches total counts of records within the data source, which will run every time a scheduled catalog is triggered and will automatically be inside. There are two features inside the catalog category, one is for tagging and labeling, and the other one for scheduling where users can select a time interval when to run it. All these changes are saved and the new catalog is shown in a list of Catalogs with some fields that have all the statistics calculated and set in the previous step, like how many tables and columns are crawled, when is the last run, and options to edit, delete and run the catalog. With run option, AdaptiveScale retrieves all the information from the data source and it applies the catalog rules set. Example Create a catalog To create a data catalog, the user must click on Catalogs tab from the menu on the left. If there are no catalogs configured, the user can click on New Catalog button to add a new catalog. Select a connection Same as in the Data source category, in Catalogs the user must fill the catalog name that should be unique, choose a data source connection, and all tables are filled automatically. There is an option to select all the table fields by choosing All or select as many as the user wants by choosing Selected tables . In the tables section, tables are represented as a tree view, where the user can see the table name and how many columns it has. When one of the tables or columns is selected the Details section is visible with all the detail of the selected table or column, such as type, number of records, number of fields and so on. That table then can be expanded and the user can see all the columns together. At the upper right corner, there are some buttons that have different functionalities. The difference between the ones at the top and the others inside the Details section is that those in the top are for the catalog schema, and those inside the Details section are applied only for the table or column selected. Extract - JSON schema of the table or the data source. AVRO - AVRO schema of the table or data source. DDL - Data definition language DDL of the table or data source All of these schema types and DDLs can be copied and used in advance by clicking on their Copy button. There is also a Prievew button that when is clicked it shows a sample of the data. Tag a column In the metadata section, at the bottom right corner, the user is able to tag a table or a column. To add a tag, the user should click on the table or on the column, add a tag name on the input field next to Tag and press on Enter . If the column is tagged, is has an icon on the column with the tag name on it. Label a column Same as for tagging, labeling is one other feature on metadata section. To add a label, the user should click on the table or on the column, add a key and value for labeling and click on + button. If the user wants to delete the label should click on - button. If the column is labeled, is has an icon on the column with the label on it. Now that the user has finished with setting a catalog it can choose whether it wants to set a schedule on the next tab or just Save and Run the catalog. Schedule a run If the user wants to add a scheduler, it should enable the add scheduler toggle, if not, it should click on Save and Run button. Once the toggle is enabled, the user can set a scheduler to run the catalog. First should be set a start date and an end date. After that, the user can choose if the scheduler can run monthly, weekly, daily, hourly or in minutes and in what hour. To save the process the user must click on Save and Run button and the catalog will be created. Upload a catalog To upload a catalog you need to upload the configured json on the upload catalog button. For preparing an external catalog json you need to define few elements in the json. The mandatory fields are: name datasourceName dataSourceId includeAllSchemas schema schedule You can also upload an external catalog directly from API by sending the json file as body in this endpoint api/catalog/third-party Here is an example of an external catalog: { \"dataSourceDescription\": \"test-csv\", \"dataSourceId\": \"3e0fe597-5cc7-4293-a887-49a5ae588961\", \"dataSourceName\": \"test-csv\", \"id\": \"2c8e4027-8537-4017-bd8e-93f1e84998fa\", \"includeAllSchemas\": false, \"lastRun\": \"\", \"lastRunId\": \"\", \"name\": \"csv-catalog\", \"schema\": { \"name\": \"titanic.csv\", \"type\": \"record\", \"fields\": [ { \"name\": \"PassengerId\", \"type\": \"int\", \"labels\": [], \"properties\": { \"description\": \"\" }, \"sourceType\": \"int\", \"tagRules\": {}, \"tags\": [] } ], \"labels\": [], \"properties\": { \"createdDate\": \"2022-10-26T11:29:09.274+02:00\", \"description\": \"\" }, \"tagRules\": {}, \"tags\": [] } } Edit a catalog Now that one catalog is configured, on Catalog tab the user can see a list of the catalogs and a button Add Catalog that redirects to the Catalogs screen for creating a new catalog. The catalog can be edited, deleted, ran and searched. When the user clicks on the pen icon next to the catalog from the list they can edit the name, tags, labels or scheduler, and save the changes. Delete a catalog The deletion of a catalog is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the catalog, and if yes the user clicks Delete and if not clicks Cancel. Run a catalog To run a catalog from the catalog list the user must click on the play icon. With that click the changes made on the catalog are saved and can be viewed in the schema evolution, data lineage. Run History Run History tab lists all the runs of the catalog. Search for a catalog The user can search for a catalog from the list of catalogs. The catalog that we created as an example is named catalog data source , so if the user searches for test there will be no results.","title":"Catalogs"},{"location":"catalogs/#example","text":"","title":"Example"},{"location":"catalogs/#create-a-catalog","text":"To create a data catalog, the user must click on Catalogs tab from the menu on the left. If there are no catalogs configured, the user can click on New Catalog button to add a new catalog.","title":"Create a catalog"},{"location":"catalogs/#select-a-connection","text":"Same as in the Data source category, in Catalogs the user must fill the catalog name that should be unique, choose a data source connection, and all tables are filled automatically. There is an option to select all the table fields by choosing All or select as many as the user wants by choosing Selected tables . In the tables section, tables are represented as a tree view, where the user can see the table name and how many columns it has. When one of the tables or columns is selected the Details section is visible with all the detail of the selected table or column, such as type, number of records, number of fields and so on. That table then can be expanded and the user can see all the columns together. At the upper right corner, there are some buttons that have different functionalities. The difference between the ones at the top and the others inside the Details section is that those in the top are for the catalog schema, and those inside the Details section are applied only for the table or column selected. Extract - JSON schema of the table or the data source. AVRO - AVRO schema of the table or data source. DDL - Data definition language DDL of the table or data source All of these schema types and DDLs can be copied and used in advance by clicking on their Copy button. There is also a Prievew button that when is clicked it shows a sample of the data.","title":"Select a connection"},{"location":"catalogs/#tag-a-column","text":"In the metadata section, at the bottom right corner, the user is able to tag a table or a column. To add a tag, the user should click on the table or on the column, add a tag name on the input field next to Tag and press on Enter . If the column is tagged, is has an icon on the column with the tag name on it.","title":"Tag a column"},{"location":"catalogs/#label-a-column","text":"Same as for tagging, labeling is one other feature on metadata section. To add a label, the user should click on the table or on the column, add a key and value for labeling and click on + button. If the user wants to delete the label should click on - button. If the column is labeled, is has an icon on the column with the label on it. Now that the user has finished with setting a catalog it can choose whether it wants to set a schedule on the next tab or just Save and Run the catalog.","title":"Label a column"},{"location":"catalogs/#schedule-a-run","text":"If the user wants to add a scheduler, it should enable the add scheduler toggle, if not, it should click on Save and Run button. Once the toggle is enabled, the user can set a scheduler to run the catalog. First should be set a start date and an end date. After that, the user can choose if the scheduler can run monthly, weekly, daily, hourly or in minutes and in what hour. To save the process the user must click on Save and Run button and the catalog will be created.","title":"Schedule a run"},{"location":"catalogs/#upload-a-catalog","text":"To upload a catalog you need to upload the configured json on the upload catalog button. For preparing an external catalog json you need to define few elements in the json. The mandatory fields are: name datasourceName dataSourceId includeAllSchemas schema schedule You can also upload an external catalog directly from API by sending the json file as body in this endpoint api/catalog/third-party Here is an example of an external catalog: { \"dataSourceDescription\": \"test-csv\", \"dataSourceId\": \"3e0fe597-5cc7-4293-a887-49a5ae588961\", \"dataSourceName\": \"test-csv\", \"id\": \"2c8e4027-8537-4017-bd8e-93f1e84998fa\", \"includeAllSchemas\": false, \"lastRun\": \"\", \"lastRunId\": \"\", \"name\": \"csv-catalog\", \"schema\": { \"name\": \"titanic.csv\", \"type\": \"record\", \"fields\": [ { \"name\": \"PassengerId\", \"type\": \"int\", \"labels\": [], \"properties\": { \"description\": \"\" }, \"sourceType\": \"int\", \"tagRules\": {}, \"tags\": [] } ], \"labels\": [], \"properties\": { \"createdDate\": \"2022-10-26T11:29:09.274+02:00\", \"description\": \"\" }, \"tagRules\": {}, \"tags\": [] } }","title":"Upload a catalog"},{"location":"catalogs/#edit-a-catalog","text":"Now that one catalog is configured, on Catalog tab the user can see a list of the catalogs and a button Add Catalog that redirects to the Catalogs screen for creating a new catalog. The catalog can be edited, deleted, ran and searched. When the user clicks on the pen icon next to the catalog from the list they can edit the name, tags, labels or scheduler, and save the changes.","title":"Edit a catalog"},{"location":"catalogs/#delete-a-catalog","text":"The deletion of a catalog is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the catalog, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete a catalog"},{"location":"catalogs/#run-a-catalog","text":"To run a catalog from the catalog list the user must click on the play icon. With that click the changes made on the catalog are saved and can be viewed in the schema evolution, data lineage.","title":"Run a catalog"},{"location":"catalogs/#run-history","text":"Run History tab lists all the runs of the catalog.","title":"Run History"},{"location":"catalogs/#search-for-a-catalog","text":"The user can search for a catalog from the list of catalogs. The catalog that we created as an example is named catalog data source , so if the user searches for test there will be no results.","title":"Search for a catalog"},{"location":"collections/","text":"Collection is the category where the user can create a collection based on the existing data sources, or can create a new one from scratch. The data of the data source can be explored on table designer, it can change or also create completely new ones. The collection can be linked to a git repo. Example Create a collection To create a collection, the user must click on Collection tab from the menu on the left. If there are no collections configured, the user can click on Add Collection button to add a new collection. When Collection screen is opened, the user must fill a collection name that should be unique, choose a data source connection if it wants to modify an existing one or choose a DMBS type to create a new one, and the tables are filled automatically. In the tables section, tables are represented as a tree view, where the user can see the table and columns names. A new table can be created as well. When one of the tables is selected the Table Designer section is visible with all the detail of the selected table such as the columns it contains, their types, the primary key and nullable columns. These details can be changed or deleted by the user. Also it is possible to add new columns and change the schema. Add a column If the user wants to restructure the table it can add a column to the desired table by clicking on the table first and then click on Add Column on the Table designer. For a new column, the user should provide a name and a type, a size and coonfigure a feature such as nullable or primary key. Add a test to a column More to add to a column will be a test with an operator. To add a test the user must hover over the column and Add Test button will be shown. By clicking on this button a new test will be added, so the user can choose an operator and a value to be compared to. To save the changes the user must click on Save button. This will also be changed on DMBL Editor 's yaml file. Add a table Another feature of Collections is the possibility to add a new table to the schema by clicking on Add Table button. On Table Designer the user should give a unique name to the table and continue adding columns as described above by clicking on Add Column button. After the user is ready with setting a collection can save it with Save Collection button. Edit a collection Now that a collection is configured, on Collection tab the user can see a list of the collections and a button Add Collection that redirects to the Collections screen for creating a new one. The collection can be edited, deleted, deployed and searched. When the user clicks on the pen icon next to the collection from the list they can edit the name, description, add a git repo, add or delete a table, add or delete a column and save the changes. Delete a collection The deletion of a collection is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the collection, and if yes the user clicks Delete and if not clicks Cancel. Deploy a collection The collection that we created or updated can be deployed on a taget DB. This is done by clicking on the Deploy button from the list. A new screen for Deployment is opened where the new schema is shown on the left of the screen, a collection that can be edited and a target DB which represents the location where the collection will be deployed. The user can choose from the list of data sources where to deploy the new collection. The last step after choosing a target DB will be to click on Deploy button so the deployment can start, and a status of the deployment with details will be shown. The actions for the deployment include download and delete.","title":"Collections"},{"location":"collections/#example","text":"","title":"Example"},{"location":"collections/#create-a-collection","text":"To create a collection, the user must click on Collection tab from the menu on the left. If there are no collections configured, the user can click on Add Collection button to add a new collection. When Collection screen is opened, the user must fill a collection name that should be unique, choose a data source connection if it wants to modify an existing one or choose a DMBS type to create a new one, and the tables are filled automatically. In the tables section, tables are represented as a tree view, where the user can see the table and columns names. A new table can be created as well. When one of the tables is selected the Table Designer section is visible with all the detail of the selected table such as the columns it contains, their types, the primary key and nullable columns. These details can be changed or deleted by the user. Also it is possible to add new columns and change the schema.","title":"Create a collection"},{"location":"collections/#add-a-column","text":"If the user wants to restructure the table it can add a column to the desired table by clicking on the table first and then click on Add Column on the Table designer. For a new column, the user should provide a name and a type, a size and coonfigure a feature such as nullable or primary key.","title":"Add a column"},{"location":"collections/#add-a-test-to-a-column","text":"More to add to a column will be a test with an operator. To add a test the user must hover over the column and Add Test button will be shown. By clicking on this button a new test will be added, so the user can choose an operator and a value to be compared to. To save the changes the user must click on Save button. This will also be changed on DMBL Editor 's yaml file.","title":"Add a test to a column"},{"location":"collections/#add-a-table","text":"Another feature of Collections is the possibility to add a new table to the schema by clicking on Add Table button. On Table Designer the user should give a unique name to the table and continue adding columns as described above by clicking on Add Column button. After the user is ready with setting a collection can save it with Save Collection button.","title":"Add a table"},{"location":"collections/#edit-a-collection","text":"Now that a collection is configured, on Collection tab the user can see a list of the collections and a button Add Collection that redirects to the Collections screen for creating a new one. The collection can be edited, deleted, deployed and searched. When the user clicks on the pen icon next to the collection from the list they can edit the name, description, add a git repo, add or delete a table, add or delete a column and save the changes.","title":"Edit a collection"},{"location":"collections/#delete-a-collection","text":"The deletion of a collection is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the collection, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete a collection"},{"location":"collections/#deploy-a-collection","text":"The collection that we created or updated can be deployed on a taget DB. This is done by clicking on the Deploy button from the list. A new screen for Deployment is opened where the new schema is shown on the left of the screen, a collection that can be edited and a target DB which represents the location where the collection will be deployed. The user can choose from the list of data sources where to deploy the new collection. The last step after choosing a target DB will be to click on Deploy button so the deployment can start, and a status of the deployment with details will be shown. The actions for the deployment include download and delete.","title":"Deploy a collection"},{"location":"dashboard/","text":"AdaptiveScale\u2019s dashboard is a representation of general statistics for how many data sources the user has crawled and how many tables and tags there are. The user has the possibility to add a data source, browse a catalog, or search for metadata tag from the dashboard. The AdaptiveScale driver can be downloaded directly from the dashboard. To assist users in using a the productg there are User Guides, Tutorials and the Documentation. Example The first thing that is shown when the user opens AdaptiveScale is the dashboard. As we can see from the picture below there are no data sources, catalogs or tags created yet in AdaptiveScale. To change the view of the dashboard, as stated earlier, the user can either add data sources or catalogs from the dashboard osr add them from the menu tabs on the left. The user can click on Data Source statistic button to create a new data source. It will redirect them to Data source screen. More on Data sources ... The user can click on Catalogs statistic button to create a new catalog and browse the data. It will redirect them to Catalogs screen. More on Catalogs ... If the user has already added a data source and a catalog then it can tag a table or column from the Business Tags category. The user can click on Tags statistic button to tagging. It will redirect them to Business Tags screen. More on Business Tags ... To return back to the dashboard, the user can click on AdaptiveScale logo.","title":"Dashboard"},{"location":"dashboard/#example","text":"The first thing that is shown when the user opens AdaptiveScale is the dashboard. As we can see from the picture below there are no data sources, catalogs or tags created yet in AdaptiveScale. To change the view of the dashboard, as stated earlier, the user can either add data sources or catalogs from the dashboard osr add them from the menu tabs on the left. The user can click on Data Source statistic button to create a new data source. It will redirect them to Data source screen. More on Data sources ... The user can click on Catalogs statistic button to create a new catalog and browse the data. It will redirect them to Catalogs screen. More on Catalogs ... If the user has already added a data source and a catalog then it can tag a table or column from the Business Tags category. The user can click on Tags statistic button to tagging. It will redirect them to Business Tags screen. More on Business Tags ... To return back to the dashboard, the user can click on AdaptiveScale logo.","title":"Example"},{"location":"datasources/","text":"Data sources is the category where the user sets, creates a data source connection. There are six options to choose from, a JDBC driver, file, file transfer protocol(FTP), Google Cloud Storage (GCS), Amazon Simple Storage Service (Amazon S3) and Microsoft Azure. The supported database servers are: MySQL, MS SQL Server, Postgres, Oracle, BigQuery, and Generic driver. To create a new data source connection the user needs to fill in the connection name, description, a host and a port for the server, database name for the utilized database server with a username and password, define a driver class name and upload a JDBC driver. All these fields are saved and the newly created data source connection is shown in a list of Data source connections, where there are options to edit and delete it. For a new file or FTP data source connection the user need to fill in the connection name, choose a URI of the file, and from the dropdown choose CSV or AVRO which should be same as the type of the file. The same applies for the other connections that are left with some small differences for example, for GCS data source connection the user must add the Service Account, for Amazon S3 the user must provide the Access Key ID and Secret Access Key and for Microsoft Azure the user should fill Account Name, Container Name and SAS Token. Example Create a data source To create a data source, the user must click on Data Source tab from the menu on the left. If there are no data sources configured, the user clicks on Add Data Source button to add a new data source. When Add Data Source button is clicked, a new screen with fifteen options to choose from for data source connection is shown. JDBC connection If the user chooses JDBC connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port based on the DB server chosen from the dropdown list, configured username and password for the data source, the JDBC driver should be uploaded and JDBC connection string should be given in a format jdbc:sqlserver://{host}[:{port}][databasename={database}] . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. File connection If the user chooses file connection type the user needs to fill a name, choose a type of the file, csv, avro or parquet, and fill the URI of the file. When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. FTP connection If the user chooses FTP connection type it needs to fill a name, choose a type, a directory, csv, avro or parquet, and fill the URI of the file starting with file:// . In order to explore what files are inside the directory by clicking on Explore the user must specify the URI. If it clicks on one of the files of that directory, the URI will be re written automatically. After clicking Save button all the input is saved and the data source is ready for use. Google Bucket connection If the user chooses Google Bucket connection type the user needs to fill a name, choose a type, a directory, csv, avro or parquet, write the service account key and fill the URI of the file starting with gs:// . The user can explore what files are inside the bucket by clicking on Explore , even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or re written automatically. After clicking Save button all the input is saved and the data source is ready for use. S3 Amazon connection If the user chooses S3 Amazon connection type the user needs to fill a name, choose a schema type of the file, csv, avro or parquet, provide access key ID and secret access key and fill the URI of the file starting with s3a:// . The user can explore what files are inside the bucket by clicking on Explore , even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or re written automatically. After clicking Save button all the input is saved and the data source is ready for use. Microsoft Azure connection If the user chooses Microsoft Azure connection type the user needs to fill a name, choose a schema type of the file, csv, avro or parquet, provide Account name, Container name and SAS token and fill the URI of the file starting with wasbs:// . The user can explore what files are inside the bucket by clicking on Explore , even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or re written automatically. After clicking Save button all the input is saved and the data source is ready for use. HDFS connection If the user chooses HDFS connection type it needs to fill a name, choose a type, a directory, csv, avro or parquet, and fill the URI of the file starting with file:// . In order to explore what files are inside the directory by clicking on Explore the user must specify the URI. If it clicks on one of the files of that directory, the URI will be re written automatically. When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. DB2 connection If the user chooses DB2 connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format jdbc:db2://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. Oracle connection If the user chooses Oracle connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format jdbc:oracle:thin:@//${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. Snowflake connection If the user chooses Snowflake connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format jdbc:snowflake://${host}:${port}/db=dbname . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. Bigquery connection If the user chooses Bigquery connection type, must fill some fields such as , connection name that should be unique, a description, database name, the driver should be uploaded and JDBC connection string should be given in a format jdbc:bigquery://{host}[:{port}][;databaseName={database}] . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. Microsoft SQL Server connection If the user chooses Microsoft SQL Server connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and connection string should be given in a format jdbc:sqlserver://{host}[:{port}][databasename={database}] . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. PostgreSQL connection If the user chooses PostgreSQL connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and connection string should be given in a format jdbc:postgresql://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. MySQL connection If the user chooses MySQL connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and connection string should be given in a format jdbc:mysql://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. Kinetica connection If the user chooses Kinetica connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, a database name, configured username and password for the data source, attributes such as key-value pairs can be added, the driver should be uploaded and connection string should be given in a format jdbc:kinetica:URL=http://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use. Edit a data source Now that one data source is configured, on Data Source tab the user can see a list of the data sources and a button Add Data Source that redirects to the Data source screen for creating a new data source. The data source can be edited, deleted and searched. When the user clicks on the pen icon next to the data sources from the list they can edit some of the fields and save the changes. Delete a data source The deletion of a data source is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the data source, and if yes the user clicks Delete and if not clicks Cancel. Search for a data source The user can search for a data source from the list of data sources. The data source that we created as an example is named data source demo , so if the user searches for test there will be no results.","title":"Data Sources"},{"location":"datasources/#example","text":"","title":"Example"},{"location":"datasources/#create-a-data-source","text":"To create a data source, the user must click on Data Source tab from the menu on the left. If there are no data sources configured, the user clicks on Add Data Source button to add a new data source. When Add Data Source button is clicked, a new screen with fifteen options to choose from for data source connection is shown.","title":"Create a data source"},{"location":"datasources/#jdbc-connection","text":"If the user chooses JDBC connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port based on the DB server chosen from the dropdown list, configured username and password for the data source, the JDBC driver should be uploaded and JDBC connection string should be given in a format jdbc:sqlserver://{host}[:{port}][databasename={database}] . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"JDBC connection"},{"location":"datasources/#file-connection","text":"If the user chooses file connection type the user needs to fill a name, choose a type of the file, csv, avro or parquet, and fill the URI of the file. When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"File connection"},{"location":"datasources/#ftp-connection","text":"If the user chooses FTP connection type it needs to fill a name, choose a type, a directory, csv, avro or parquet, and fill the URI of the file starting with file:// . In order to explore what files are inside the directory by clicking on Explore the user must specify the URI. If it clicks on one of the files of that directory, the URI will be re written automatically. After clicking Save button all the input is saved and the data source is ready for use.","title":"FTP connection"},{"location":"datasources/#google-bucket-connection","text":"If the user chooses Google Bucket connection type the user needs to fill a name, choose a type, a directory, csv, avro or parquet, write the service account key and fill the URI of the file starting with gs:// . The user can explore what files are inside the bucket by clicking on Explore , even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or re written automatically. After clicking Save button all the input is saved and the data source is ready for use.","title":"Google Bucket connection"},{"location":"datasources/#s3-amazon-connection","text":"If the user chooses S3 Amazon connection type the user needs to fill a name, choose a schema type of the file, csv, avro or parquet, provide access key ID and secret access key and fill the URI of the file starting with s3a:// . The user can explore what files are inside the bucket by clicking on Explore , even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or re written automatically. After clicking Save button all the input is saved and the data source is ready for use.","title":"S3 Amazon connection"},{"location":"datasources/#microsoft-azure-connection","text":"If the user chooses Microsoft Azure connection type the user needs to fill a name, choose a schema type of the file, csv, avro or parquet, provide Account name, Container name and SAS token and fill the URI of the file starting with wasbs:// . The user can explore what files are inside the bucket by clicking on Explore , even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or re written automatically. After clicking Save button all the input is saved and the data source is ready for use.","title":"Microsoft Azure connection"},{"location":"datasources/#hdfs-connection","text":"If the user chooses HDFS connection type it needs to fill a name, choose a type, a directory, csv, avro or parquet, and fill the URI of the file starting with file:// . In order to explore what files are inside the directory by clicking on Explore the user must specify the URI. If it clicks on one of the files of that directory, the URI will be re written automatically. When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"HDFS connection"},{"location":"datasources/#db2-connection","text":"If the user chooses DB2 connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format jdbc:db2://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"DB2 connection"},{"location":"datasources/#oracle-connection","text":"If the user chooses Oracle connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format jdbc:oracle:thin:@//${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"Oracle connection"},{"location":"datasources/#snowflake-connection","text":"If the user chooses Snowflake connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format jdbc:snowflake://${host}:${port}/db=dbname . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"Snowflake connection"},{"location":"datasources/#bigquery-connection","text":"If the user chooses Bigquery connection type, must fill some fields such as , connection name that should be unique, a description, database name, the driver should be uploaded and JDBC connection string should be given in a format jdbc:bigquery://{host}[:{port}][;databaseName={database}] . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"Bigquery connection"},{"location":"datasources/#microsoft-sql-server-connection","text":"If the user chooses Microsoft SQL Server connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and connection string should be given in a format jdbc:sqlserver://{host}[:{port}][databasename={database}] . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"Microsoft SQL Server connection"},{"location":"datasources/#postgresql-connection","text":"If the user chooses PostgreSQL connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and connection string should be given in a format jdbc:postgresql://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"PostgreSQL connection"},{"location":"datasources/#mysql-connection","text":"If the user chooses MySQL connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, configured username and password for the data source, the driver should be uploaded and connection string should be given in a format jdbc:mysql://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"MySQL connection"},{"location":"datasources/#kinetica-connection","text":"If the user chooses Kinetica connection type, must fill some fields such as , connection name that should be unique, a description, hostname and port, a database name, configured username and password for the data source, attributes such as key-value pairs can be added, the driver should be uploaded and connection string should be given in a format jdbc:kinetica:URL=http://${host}:${port} . When the user fills all the fields the connection can be tested by clicking the button Test that will turn green if the connection is correct, and turn red if the connection is wrong. After clicking Save button all the input is saved and the data source is ready for use.","title":"Kinetica connection"},{"location":"datasources/#edit-a-data-source","text":"Now that one data source is configured, on Data Source tab the user can see a list of the data sources and a button Add Data Source that redirects to the Data source screen for creating a new data source. The data source can be edited, deleted and searched. When the user clicks on the pen icon next to the data sources from the list they can edit some of the fields and save the changes.","title":"Edit a data source"},{"location":"datasources/#delete-a-data-source","text":"The deletion of a data source is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the data source, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete a data source"},{"location":"datasources/#search-for-a-data-source","text":"The user can search for a data source from the list of data sources. The data source that we created as an example is named data source demo , so if the user searches for test there will be no results.","title":"Search for a data source"},{"location":"datatransfer/","text":"Data transfer is the category where the user can transfer data from a created catalog to Google Storage Bucket or Amazon Simple Storage Service (Amazon S3) datas source. Example Start a Data transfer To do the data trasfer the user must choose a catalog from where it wants to transfer data to a selected target which can be a GCS or S3 bucket. The path is where this transfer will be located.","title":"Data Transfer"},{"location":"datatransfer/#example","text":"","title":"Example"},{"location":"datatransfer/#start-a-data-transfer","text":"To do the data trasfer the user must choose a catalog from where it wants to transfer data to a selected target which can be a GCS or S3 bucket. The path is where this transfer will be located.","title":"Start a Data transfer"},{"location":"installation/","text":"Quickest way to deploy AdaptiveScale is to use the latest docker image. The dokcer image includes all the dependencies required to run AdaptiveScale. To start AdaptiveScale docker container simply the run the following docker command: docker run -p 8080:8080 -e ACCEPT_EULA=Y adaptivescale/proedms:latest This will fetch docker image with newest published AdaptiveScale version. The ACCEPT_EULA env variable is mandatory for confirming your acceptance of the End-User Licensing Agreement. After the docker image is deployed AdaptiveScale can be accesed via browser from the following link: http://localhost:8080/ One of the dependencies of AdaptiveScale is ElasticSearch - currently a version of it is bundled inside. Container can be set to connect to external ElasticSearch by setting the following env variables: ADAPTIVESCALE_ELASTIC_HOST . Example on how to set it is below: docker run -p 8080:8080 -e ADAPTIVESCALE_ELASTIC_HOST=http://localhost:9200 adaptivescale/proedms:latest AdaptiveScale comes with default username set as admin as well as password set as admin . This can also be changed by providing the following env variables: PROEDMS_USER and PROEDMS_PASSWORD . Example on how to use this is below: docker run -p 8080:8080 -e PROEDMS_USER=testuser -e PROEDMS_PASSWORD=testpassword adaptivescale/proedms:latest","title":"Installation"},{"location":"schedules/","text":"Schedules is the category where all the scheduled catalogs are listed, with information like the catalog name, last run, next scheduled run, and options to activate or deactivate a schedule, and the option to delete it. Example Create a schedule To create a schedule, the user must click on Schedules tab from the menu on the left or whenever the user creates a Catalog. If there are no schedules created, the user clicks on Add Schedule button to add a new schedule, and select a catalog from the list which redirects the user to the Schedule tab of the chosen catalog to set a schedule. Once the user schedules a run for a catalog, in the Catalogs tab, it is listed in the Schedules category. The list of schedules has information like the catalog name, schedule, last run and next run. It also has the possibility to delete and activate or deactivate a schedule. Activate schedule When the user schedules a catalog, by default it is listed in Active scheduled list. This means that the next run of the catalog will happen on the date schduled in the list. If the user wants to stop the next run, it can deactivate the catalog run by clicking on the eye icon. Deactivate schedule When the user deactivates a catalog schedules run, by clicking on deactivate button, that catalog then will be listed in Inactive scheduled list. This means that the next run of the catalog will not happen on the date schduled in the list until it is activated again. If the user wants to activate the catalog run it is done by clicking on the eye icon. Delete a schedule The deletion of a scheduled catalog run is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the schedule, and if yes the user clicks Delete and if not clicks Cancel.","title":"Schedules"},{"location":"schedules/#example","text":"","title":"Example"},{"location":"schedules/#create-a-schedule","text":"To create a schedule, the user must click on Schedules tab from the menu on the left or whenever the user creates a Catalog. If there are no schedules created, the user clicks on Add Schedule button to add a new schedule, and select a catalog from the list which redirects the user to the Schedule tab of the chosen catalog to set a schedule. Once the user schedules a run for a catalog, in the Catalogs tab, it is listed in the Schedules category. The list of schedules has information like the catalog name, schedule, last run and next run. It also has the possibility to delete and activate or deactivate a schedule.","title":"Create a schedule"},{"location":"schedules/#activate-schedule","text":"When the user schedules a catalog, by default it is listed in Active scheduled list. This means that the next run of the catalog will happen on the date schduled in the list. If the user wants to stop the next run, it can deactivate the catalog run by clicking on the eye icon.","title":"Activate schedule"},{"location":"schedules/#deactivate-schedule","text":"When the user deactivates a catalog schedules run, by clicking on deactivate button, that catalog then will be listed in Inactive scheduled list. This means that the next run of the catalog will not happen on the date schduled in the list until it is activated again. If the user wants to activate the catalog run it is done by clicking on the eye icon.","title":"Deactivate schedule"},{"location":"schedules/#delete-a-schedule","text":"The deletion of a scheduled catalog run is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the schedule, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete a schedule"},{"location":"schemaevolution/","text":"Schema evolution is the category where the user can choose the catalog and can see the changes made over time to that catalog. The data lineage can be seen very clearly, for example when the user selects a date, the column that has been changed is highlighted with orange color or if a column is deleted is highlighted with red color. Example To view the catalog changes over time, the user must click on Schema Evolution category tab and choose a catalog that wants to view the data lineage. When the user clicks on the catalog, a new graphical view of all the runs is shown. The graph points represent the data lineage in time when that catalog was run. With this feature, the user is able to compare the catalog changes during specific times. For example, in the picture below, during the first run, version 1.0 there were 23 fields and in the second run there are only 2 fields, where 21 fields are deleted and are colored with red. If the user hovers on the point of change in the graph there will be a detailed view of what was changed during the runs. Now if the user goes to the Catalogs, add some fields and runs the catalog, it can see the new run in the data lineage, version 1.2. From two runs that were in the previous example, now there are three v1.0, v1.1 and v1.2. The new columns that are added are colored with green and the number of fields from 2 became 4. Lastly, if the user wants to make a change in the current schema, add a tag or remove one, it needs to return back to Catalogs, make the changes and run the catalog again. From three runs that were in the previous example, now there will be four. The colummns that are modified are colored with orange and the user can see the tag and label icon on the columns. All these changes in one are captured in the picture below. Column actor is deleted and colored with red, column address is modified, has removed label and colored with orange, and colum country is added to the schema and colored with green. To view more details the user can expand the list of changes and can see them described.","title":"Schema Evolution"},{"location":"schemaevolution/#example","text":"To view the catalog changes over time, the user must click on Schema Evolution category tab and choose a catalog that wants to view the data lineage. When the user clicks on the catalog, a new graphical view of all the runs is shown. The graph points represent the data lineage in time when that catalog was run. With this feature, the user is able to compare the catalog changes during specific times. For example, in the picture below, during the first run, version 1.0 there were 23 fields and in the second run there are only 2 fields, where 21 fields are deleted and are colored with red. If the user hovers on the point of change in the graph there will be a detailed view of what was changed during the runs. Now if the user goes to the Catalogs, add some fields and runs the catalog, it can see the new run in the data lineage, version 1.2. From two runs that were in the previous example, now there are three v1.0, v1.1 and v1.2. The new columns that are added are colored with green and the number of fields from 2 became 4. Lastly, if the user wants to make a change in the current schema, add a tag or remove one, it needs to return back to Catalogs, make the changes and run the catalog again. From three runs that were in the previous example, now there will be four. The colummns that are modified are colored with orange and the user can see the tag and label icon on the columns. All these changes in one are captured in the picture below. Column actor is deleted and colored with red, column address is modified, has removed label and colored with orange, and colum country is added to the schema and colored with green. To view more details the user can expand the list of changes and can see them described.","title":"Example"},{"location":"search/","text":"Search is the category where the user can search by tags, field names, table names, data source names, and more. Here\u2019s a list of supported filters: string, wild card: *(string)* wild card: *(string)* tag: <tag name (string)> key: <key name (string)> val: <(string)> If for example, the user searches for a tag name, it goes to Elastic search and retrieves that tag with how many matches it finds. The user can also see the data lineage from the search section when clicking on the column. Example In the Metadata Search tab, the user can search by tags, field names, table names, data source names, and more. Let's do a search for each of them. From earlier examples we can recall that we tagged a colum with a tag named pii , so if we search for pii we will have a result. Now that the user searched for pii , the results are shown in a form of a list, where a dashboard is shown above it. The dashboard shows a statistic of how many data sources, records, fields, tags and labels are crawled from that search.","title":"Metadata Search"},{"location":"search/#example","text":"In the Metadata Search tab, the user can search by tags, field names, table names, data source names, and more. Let's do a search for each of them. From earlier examples we can recall that we tagged a colum with a tag named pii , so if we search for pii we will have a result. Now that the user searched for pii , the results are shown in a form of a list, where a dashboard is shown above it. The dashboard shows a statistic of how many data sources, records, fields, tags and labels are crawled from that search.","title":"Example"},{"location":"security/","text":"When it comes to security AdaptiveScale is divided in two sections: Secure store - for storing credentials and other secrutiy related info used during operations like accessing data source. Authentication (LDAP) - for allowing/restricting acces to the platform. Secure Store To secure credentials platform utilizes Java KeyStore in JKS format to create a secure store file. Location of this file and the password used for it's encryption can be configured in application.properties file. Note: these values need to be modified before any data source is added - otherwise all data source passwords for existing data sources need to be re-entered through UI->Data Source->Edit form. Property Value Required Description proedms.secure.store.file keystore.jks Yes Location of the file where to store keys proedms.secure.store.type JCEKS Yes Type of the keystore. Currently only JCEKS is supported Authentication (LDAP) Authentication in AdaptiveScale is optional and can be enabled/disabled from application.properties file. AdaptiveScale uses LDAP for authentication and it also provides the ability to bring up local server based on provided LDIFF file. Once the user credentials are verified JWT token is created which is valid for 24 hours and all subsequent interactions have this token provided in Authorization header property. There are few properties that are required to be set in application.properties in order to enable authentication. Property Value Required Description proedms.security.authentication.enabled true Y This property enables the authenticaiton mechanism in general. Default: false proedms.jwt.auth.type SECRET Y Indicates the type of secret JWT will use to has/ecnrypt tokens. Can be set to SECRET for using string type of key. Can be set to KEY for using private key file. proedms.jwt.secret [String/Path] Y Depending on how the property proedms.jwt.auth.type this can be either a string type of secret or location to private key file. ldap.user.dn.patterns ex:uid={0},ou=people Y User DN Pattern - A DN pattern that can used to directly login users to the LDAP database. This pattern is used for creating a DN string for \"direct\" user authentication, where the pattern is relative to the base DN in the ldapUrl. Note : uid needs to have {0} as value as it will be matched against username provided during login. ldap.group.search.base ex: ou=groups Y Subtree to search for username ldap.url ex: ldap://localhost:8389/dc=AdaptiveScale,dc=com Y LDAP url to connect to - can be set as localhost if AdaptiveScale embeded LDAP is to be used. ldap.password.attribute userPassword Y Name of the attribute for password in LDAP. Default: userPassword . AdaptiveScale also provided embeded LDAP server that can be activated by providing additional properties to properties file. This is useful for the cases where users don't have their own LDAP server or don't want to connect the platform with existing LDAP server. Keeping the configuration sa examples from abvoe and additionally setting the following configurations will allows users to start AdaptiveScale embedded LDAP server based on provided LDIFF file which will contains LDAP data. Property Value Required Description spring.ldap.embedded.ldif file:///opt/apt/auth.ldif Y Absolute path to LDIFF file spring.ldap.embedded.base-dn dc=AdaptiveScale,dc=com Y A base dn from where the server will search for users. This is dependent with LDIFF contents and is also needed in ldap.url property. spring.ldap.embedded.port 8389 Y Port on which embedded LDAP server will run. Using embedded server requires LDIFF file which is a copy of LDAP data structure dumped to single file. The following example ldiff matches the example configurations given above and creates user with username admin and password admin . Create a file with .ldif extension and add the follwing as it's content. # LDAP Auth data dn: dc=AdaptiveScale,dc=com objectclass: top objectclass: domain objectclass: extensibleObject dc: AdaptiveScale # Organizational Units dn: ou=groups,dc=AdaptiveScale,dc=com objectclass: top objectclass: organizationalUnit ou: groups dn: ou=people,dc=AdaptiveScale,dc=com objectclass: top objectclass: organizationalUnit ou: people # Accounts dn: uid=admin,ou=people,dc=AdaptiveScale,dc=com objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson cn: AdaptiveScale sn: AdaptiveScale uid: admin userPassword: admin # Create Group Admin dn: cn=admins,ou=groups,dc=AdaptiveScale,dc=com objectclass: top objectclass: groupOfUniqueNames cn: admins ou: admin uniqueMember: uid=admin,ou=people,dc=AdaptiveScale,dc=com Starting AdaptiveScale with the configration set as above will land user in Login screen where user admin and password admin can be used to authenticate. Authorization (LDAP) AdaptiveScale utilizes groups/roles for ACL, both authentication and authorization is based on LDAP, this can be configured either from the auth.ldif file or from a LDAP server. Access is divided into the following categories: Category name Category description DS Stands for data-source - handles access for resources under data-source CAT Stands for catalog - handles access for resources under catalog DEP Stands for deployment - handles access for resources under deployment COL Stands for collection - handles access for resources under collection DT Stands for data-transfer - handles access for resources under data-transfer TAG Stands for tag - handles access for resources under tag SCHED Stands for schedule - handles access for resources under schedule SEARCH Stands for search - handles access for resources under search EV Stands for schema-evolution - handles access for resources under schema-evolution ALERT Stands for alert - handles access for resources under alert SQLEXPL Stands for sql-explorer - handles access for resources under sql-explorer GIT Stands for git-configuration - handles access for resources under git-configuration Besides the categories there is also operational level permissions: Permission name Permission description READ This permission allows the user to view/list resource(s) WRITE This permission allows the user to modify resource(s) EXECUTE This permission allows the user to execute operations on resource(s) ACL in AdaptiveScale is achieved by creating a group name that combines the category name as a prefix and permission name as a suffix. Like the following example defines a group which has permission to view/list data-sources and assigns the user test as a member of the group which will allow user to have access to the data-sources: # GROUP DS-READ dn: cn=adaptive-ds-readers,ou=groups,dc=adaptivescale,dc=com objectClass: top objectClass: groupOfNames cn: adaptive-ds-read member: uid=test,ou=people,dc=adaptivescale,dc=com Besides the combinations that can be achieved above there is also an adaptive-admin group, members of which have full access to all resources, including the capability to delete resources. Example given below shows how an adaptive-admin group looks like: # GROUP ADMIN dn: cn=adaptive-admins,ou=groups,dc=adaptivescale,dc=com objectClass: top objectClass: groupOfNames cn: adaptive-admin member: uid=admin,ou=people,dc=adaptivescale,dc=com Note: Enabling and disabling permissions reflects on the UI components visibility as well.","title":"Security"},{"location":"security/#secure-store","text":"To secure credentials platform utilizes Java KeyStore in JKS format to create a secure store file. Location of this file and the password used for it's encryption can be configured in application.properties file. Note: these values need to be modified before any data source is added - otherwise all data source passwords for existing data sources need to be re-entered through UI->Data Source->Edit form. Property Value Required Description proedms.secure.store.file keystore.jks Yes Location of the file where to store keys proedms.secure.store.type JCEKS Yes Type of the keystore. Currently only JCEKS is supported","title":"Secure Store"},{"location":"security/#authentication-ldap","text":"Authentication in AdaptiveScale is optional and can be enabled/disabled from application.properties file. AdaptiveScale uses LDAP for authentication and it also provides the ability to bring up local server based on provided LDIFF file. Once the user credentials are verified JWT token is created which is valid for 24 hours and all subsequent interactions have this token provided in Authorization header property. There are few properties that are required to be set in application.properties in order to enable authentication. Property Value Required Description proedms.security.authentication.enabled true Y This property enables the authenticaiton mechanism in general. Default: false proedms.jwt.auth.type SECRET Y Indicates the type of secret JWT will use to has/ecnrypt tokens. Can be set to SECRET for using string type of key. Can be set to KEY for using private key file. proedms.jwt.secret [String/Path] Y Depending on how the property proedms.jwt.auth.type this can be either a string type of secret or location to private key file. ldap.user.dn.patterns ex:uid={0},ou=people Y User DN Pattern - A DN pattern that can used to directly login users to the LDAP database. This pattern is used for creating a DN string for \"direct\" user authentication, where the pattern is relative to the base DN in the ldapUrl. Note : uid needs to have {0} as value as it will be matched against username provided during login. ldap.group.search.base ex: ou=groups Y Subtree to search for username ldap.url ex: ldap://localhost:8389/dc=AdaptiveScale,dc=com Y LDAP url to connect to - can be set as localhost if AdaptiveScale embeded LDAP is to be used. ldap.password.attribute userPassword Y Name of the attribute for password in LDAP. Default: userPassword . AdaptiveScale also provided embeded LDAP server that can be activated by providing additional properties to properties file. This is useful for the cases where users don't have their own LDAP server or don't want to connect the platform with existing LDAP server. Keeping the configuration sa examples from abvoe and additionally setting the following configurations will allows users to start AdaptiveScale embedded LDAP server based on provided LDIFF file which will contains LDAP data. Property Value Required Description spring.ldap.embedded.ldif file:///opt/apt/auth.ldif Y Absolute path to LDIFF file spring.ldap.embedded.base-dn dc=AdaptiveScale,dc=com Y A base dn from where the server will search for users. This is dependent with LDIFF contents and is also needed in ldap.url property. spring.ldap.embedded.port 8389 Y Port on which embedded LDAP server will run. Using embedded server requires LDIFF file which is a copy of LDAP data structure dumped to single file. The following example ldiff matches the example configurations given above and creates user with username admin and password admin . Create a file with .ldif extension and add the follwing as it's content. # LDAP Auth data dn: dc=AdaptiveScale,dc=com objectclass: top objectclass: domain objectclass: extensibleObject dc: AdaptiveScale # Organizational Units dn: ou=groups,dc=AdaptiveScale,dc=com objectclass: top objectclass: organizationalUnit ou: groups dn: ou=people,dc=AdaptiveScale,dc=com objectclass: top objectclass: organizationalUnit ou: people # Accounts dn: uid=admin,ou=people,dc=AdaptiveScale,dc=com objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson cn: AdaptiveScale sn: AdaptiveScale uid: admin userPassword: admin # Create Group Admin dn: cn=admins,ou=groups,dc=AdaptiveScale,dc=com objectclass: top objectclass: groupOfUniqueNames cn: admins ou: admin uniqueMember: uid=admin,ou=people,dc=AdaptiveScale,dc=com Starting AdaptiveScale with the configration set as above will land user in Login screen where user admin and password admin can be used to authenticate.","title":"Authentication (LDAP)"},{"location":"security/#authorization-ldap","text":"AdaptiveScale utilizes groups/roles for ACL, both authentication and authorization is based on LDAP, this can be configured either from the auth.ldif file or from a LDAP server. Access is divided into the following categories: Category name Category description DS Stands for data-source - handles access for resources under data-source CAT Stands for catalog - handles access for resources under catalog DEP Stands for deployment - handles access for resources under deployment COL Stands for collection - handles access for resources under collection DT Stands for data-transfer - handles access for resources under data-transfer TAG Stands for tag - handles access for resources under tag SCHED Stands for schedule - handles access for resources under schedule SEARCH Stands for search - handles access for resources under search EV Stands for schema-evolution - handles access for resources under schema-evolution ALERT Stands for alert - handles access for resources under alert SQLEXPL Stands for sql-explorer - handles access for resources under sql-explorer GIT Stands for git-configuration - handles access for resources under git-configuration Besides the categories there is also operational level permissions: Permission name Permission description READ This permission allows the user to view/list resource(s) WRITE This permission allows the user to modify resource(s) EXECUTE This permission allows the user to execute operations on resource(s) ACL in AdaptiveScale is achieved by creating a group name that combines the category name as a prefix and permission name as a suffix. Like the following example defines a group which has permission to view/list data-sources and assigns the user test as a member of the group which will allow user to have access to the data-sources: # GROUP DS-READ dn: cn=adaptive-ds-readers,ou=groups,dc=adaptivescale,dc=com objectClass: top objectClass: groupOfNames cn: adaptive-ds-read member: uid=test,ou=people,dc=adaptivescale,dc=com Besides the combinations that can be achieved above there is also an adaptive-admin group, members of which have full access to all resources, including the capability to delete resources. Example given below shows how an adaptive-admin group looks like: # GROUP ADMIN dn: cn=adaptive-admins,ou=groups,dc=adaptivescale,dc=com objectClass: top objectClass: groupOfNames cn: adaptive-admin member: uid=admin,ou=people,dc=adaptivescale,dc=com Note: Enabling and disabling permissions reflects on the UI components visibility as well.","title":"Authorization (LDAP)"},{"location":"sqlexplorer/","text":"SQL Explorer is the category that helps users extract pieces of information from the data sources. The first things seen are a dropdown to select a data source from where the user pulls information from and the SQL Explorer, where the user can input a structured query language-formatted query. Run Query button that will run the query inputted in the editor. Limit dropdown that defines the limit of the results. The values of the dropdown are 10, 100 and 1000. When the user chooses a data source from the dropdown, all the tables and columns of that data source are shown in the left corner of the screen. It is shown as a tree view and the tables can be expanded or collapsed. This is helpful for the users to check whats on their data source and query the information easier. Example Since the user hasn't run a query yet, no results are shown. Let\u2019s try running some queries and see how should the results table looks. For this example, we\u2019ll try to get all the actors . As with every information-gathering SQL query, start with the command \u201cSELECT\u201d to declare what fields you want to see in the results table. The next command \u201cFROM\u201d will declare from which source you want to extract data. After clicking Run Query button the result table will be shown. Below the SQL editor is the results table populated with results from the command we ran. The results may contain several records which will be displayed on pages displaying as many records as we choose from the limit dropdown.","title":"SQL Explorer"},{"location":"sqlexplorer/#example","text":"Since the user hasn't run a query yet, no results are shown. Let\u2019s try running some queries and see how should the results table looks. For this example, we\u2019ll try to get all the actors . As with every information-gathering SQL query, start with the command \u201cSELECT\u201d to declare what fields you want to see in the results table. The next command \u201cFROM\u201d will declare from which source you want to extract data. After clicking Run Query button the result table will be shown. Below the SQL editor is the results table populated with results from the command we ran. The results may contain several records which will be displayed on pages displaying as many records as we choose from the limit dropdown.","title":"Example"},{"location":"tags/","text":"Tags is the category where the user can create a business tag. To create a business tag, the user needs to fill a tag name, description and a collection name. Other possibility to add a tag is to import from a csv. Example Create a business tag To create a business tag, the user must click on Tags tab from the menu on the left. If there are no tags created, the user clicks on Add Business Tag button to add a new tag, or clicks on Import From CSV that is an option to import a csv file as a new tag. Business tag When Add Business Tag button is clicked, the user must fill the tag name with a unique name, a description for that tag and a collection name. Matching pattern is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. In the input field of matching pattern we can give a name of a data source column, field or the data source it self to tag it accordingly. In the previous section in Data sources we can recall that we had a column named actor . Let us use that column for an example with matching pattern. In the matching pattern input field, we put the name of the column from the data source actor* which means that all tables or columns that have actor in their name will be tagged with pii tag. We can see the result when we run the catalog, on the Catalog tab or Schema evolution. So table actor , actor_genre and column actor_id are all tagged with pii tag. Business rules are used for applying rules like masking, on the data itself for all the fields that have the given matching tag. After clicking Save button all the input is saved and the business tag is created. Import CSV Business tags can be added also by uploading CSV files. The CSV file should have the following columns: Collection , Tag , Pattern and Description , for adding multiple matching rules in Pattern column the user should sepperate them with space. The CSV file: The Business Tags table: Edit a business tag Now that one business tag is created, on Tags tab the user can see a list of the tags and a buttons as Add Business Tag and Import from CSV that redirects to the Tags screen for creating a new business tag. The tag can be edited, deleted, activated or deactivated and searched. When the user clicks on the pen icon next to the tag from the list it can edit some of the fields and save the changes. Delete a business tag The deletion of a tag is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the tag, and if yes the user clicks Delete and if not clicks Cancel. Search for a business tag The user can search for a tag from the list of business tags. The tag that we created as an example is named tag , so if the user searches for test there will be no results. Activate a tag When the user creates a tag, by default it is listed in Active tags list. If the user wants to deactivate the tag it can click on the eye icon. Deactivate a tag When the user deactivates a tag, by clicking on deactivate button, that tag then will be listed in Inactive tags list. If the user wants to activate the tag it is done by clicking on the eye icon.","title":"Tags"},{"location":"tags/#example","text":"","title":"Example"},{"location":"tags/#create-a-business-tag","text":"To create a business tag, the user must click on Tags tab from the menu on the left. If there are no tags created, the user clicks on Add Business Tag button to add a new tag, or clicks on Import From CSV that is an option to import a csv file as a new tag.","title":"Create a business tag"},{"location":"tags/#business-tag","text":"When Add Business Tag button is clicked, the user must fill the tag name with a unique name, a description for that tag and a collection name. Matching pattern is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. In the input field of matching pattern we can give a name of a data source column, field or the data source it self to tag it accordingly. In the previous section in Data sources we can recall that we had a column named actor . Let us use that column for an example with matching pattern. In the matching pattern input field, we put the name of the column from the data source actor* which means that all tables or columns that have actor in their name will be tagged with pii tag. We can see the result when we run the catalog, on the Catalog tab or Schema evolution. So table actor , actor_genre and column actor_id are all tagged with pii tag. Business rules are used for applying rules like masking, on the data itself for all the fields that have the given matching tag. After clicking Save button all the input is saved and the business tag is created.","title":"Business tag"},{"location":"tags/#import-csv","text":"Business tags can be added also by uploading CSV files. The CSV file should have the following columns: Collection , Tag , Pattern and Description , for adding multiple matching rules in Pattern column the user should sepperate them with space. The CSV file: The Business Tags table:","title":"Import CSV"},{"location":"tags/#edit-a-business-tag","text":"Now that one business tag is created, on Tags tab the user can see a list of the tags and a buttons as Add Business Tag and Import from CSV that redirects to the Tags screen for creating a new business tag. The tag can be edited, deleted, activated or deactivated and searched. When the user clicks on the pen icon next to the tag from the list it can edit some of the fields and save the changes.","title":"Edit a business tag"},{"location":"tags/#delete-a-business-tag","text":"The deletion of a tag is done by clicking on the bin icon. When the icon is clicked, a pop up appears with a question if the user is sure about deleting the tag, and if yes the user clicks Delete and if not clicks Cancel.","title":"Delete a business tag"},{"location":"tags/#search-for-a-business-tag","text":"The user can search for a tag from the list of business tags. The tag that we created as an example is named tag , so if the user searches for test there will be no results.","title":"Search for a business tag"},{"location":"tags/#activate-a-tag","text":"When the user creates a tag, by default it is listed in Active tags list. If the user wants to deactivate the tag it can click on the eye icon.","title":"Activate a tag"},{"location":"tags/#deactivate-a-tag","text":"When the user deactivates a tag, by clicking on deactivate button, that tag then will be listed in Inactive tags list. If the user wants to activate the tag it is done by clicking on the eye icon.","title":"Deactivate a tag"}]}