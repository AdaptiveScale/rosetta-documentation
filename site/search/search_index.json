{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p> Data Management, powered by Metadata! </p> <p>Data is a key asset for enterprises, yet finding the right data on the right systems can be elusive. Rosetta  allows you to leverage the metadata from your data sources to discover and organize data. Rosetta  helps catalog all the data sources and files that a user wants to discover and centralizes that metadata in a single location. Catalog generation with tagging and labeling is supported down to the column level, and the scheduling is customizable for catalog triggers. Scheduling is an important part of the cataloging process because as data is mutated in respective data sources something needs to keep track of what changes are taking place and captures that lineage. All your metadata is searchable and it's powered by ElasticSearch. Users can also query and browse any JDBC-compliant database in SQL Explorer. You can also use your catalog as a curated source for transferring the data, with associated filtering and governance policies applied to cloud storage or Hadoop HDFS. Cloud storage support includes Amazon S3, Azure Data Lake Storage, and Google Cloud Storage.</p>"},{"location":"#categories","title":"Categories","text":"<p>The dashboard provides general statistics for how many data sources the user has crawled, how many catalogs have been created, and how many tags or labels are created. The Rosetta  JDBC driver, which you can download directly from the dashboard, allows you to connect to Rosetta  using your favorite JDBC-compliant development environment to browse and query cataloged data sources with SQL. To help the users get more familiar with Rosetta, there are links to User Guides, Tutorials, and Documentation. The user interface is organized into the following categories: Data Sources, Catalogs, Tags, Schedules, Schema Evolution, Search, SQL Explorer, Data Transfer, and Collections.</p>"},{"location":"#data-sources","title":"Data sources","text":"<p>Data Sources is the category where the user defines a data source connection. There are six options to choose from; a JDBC driver, file, file transfer protocol(FTP), Google Bucket, Amazon S3, and Microsoft Azure. The supported database servers are MySQL, MS SQL Server, Postgres, Oracle, BigQuery, and Generic driver. The Generic driver expands the database list to any JDBC-compliant database so you can connect to any database that provides a JDBC driver. To create a new data source connection fill in the connection name, description, host and port for the server, database name for the utilized database server with a username and password, define a driver class name, and upload a JDBC driver. All these fields are saved and the newly created data source connection is shown in a list of Data Source connections. Once saved the connection will be displayed in a list where you have the option to edit or delete it. For a new file or FTP data source connection fill in the connection name, choose the URI of the file, and from the dropdown choose CSV or AVRO, which should be the same as the type of the file. The same applies to the other connections that are left with some small differences, for example, for the Google Bucket data source connection you must add the Service Account, for Amazon S3 you must provide the Access Key ID and Secret Access Key and for Microsoft Azure, you should provide Account Name, Container Name, and SAS Token.</p>"},{"location":"#catalogs","title":"Catalogs","text":"<p>Catalogs is where you create the catalog based on the metadata of the data source. In catalog generation, tagging and labeling are supported. To create a new catalog the user needs to fill in the catalog name, which needs to be a unique name to prevent any kind of naming collisions, choose a data source from existing ones or the newly created one from where all the columns and tables are going to be retrieved. In the Details section, you have the option to fetch the total counts of records within the data source. Record counts are fetched every time a scheduled catalog is triggered and will automatically be included in the catalog metadata. There are two important features inside the catalog category, one is for tagging and labeling, and the other is for scheduling, where you can select a time interval for when the catalog crawler should be run. All these changes are saved and the new catalog is shown in a list of catalogs with fields that have all the statistics calculated and set in the previous step, including how many tables and columns were crawled, when it was last run, next run, and options to edit, delete and run the catalog. With the run option, the crawler retrieves all the information from the data source and it applies the catalog rules set.</p>"},{"location":"#business-rules","title":"Business Rules","text":"<p>Business Rules allow you to automatically tag tables and columns with pattern matching. To create a business tag, the user needs to provide a tag name, description, and optionally a collection name. In the tags section, you can also apply matching patterns to tag more than one column with the same characteristics and also apply Business Rules for data governance by masking and filtering data using JavaScript code blocks. You can also define your tags externally and import them as a CSV file.</p>"},{"location":"#schedules","title":"Schedules","text":"<p>Schedules list all active and inactive crawlers defined for catalogs, with information like the catalog name, last run, next scheduled run, options to activate or deactivate a schedule, and the option to delete it.</p>"},{"location":"#schema-evolution","title":"Schema evolution","text":"<p>Schema evolution allows you to choose a catalog and see the changes that have been made to that catalog over time. The lineage can be seen very clearly; for example when the user selects a date, the column or table that has been changed is highlighted with highlight colors.</p>"},{"location":"#search","title":"Search","text":"<p>Search screen allows you to search by tags, field names, table names, data source names, and more. Here\u2019s a list of supported filters: string, wild card:</p> <ul> <li><code>*(string)*</code></li> <li><code>tag: &lt;tag name (string)&gt;</code></li> <li><code>key: &lt;key name (string)&gt;</code></li> <li><code>val: &lt;(string)&gt;</code></li> <li><code>table: &lt;table name&gt;</code></li> <li><code>column: &lt;field name&gt;</code></li> <li><code>data source: &lt;data source name&gt;</code></li> </ul> <p>If for example, you searche for a tag name, it retrieves that tag with how many matches it finds. The user can also see the data lineage from the search section when clicking on the column.</p> <p>To understand more about it, see the sections on the right.</p>"},{"location":"#sql-explorer","title":"SQL Explorer","text":"<p>SQL Explorer allows you to choose a JDBC data source connection, view the schema, and query or browse any JDBC-compliant database in the SQL.</p>"},{"location":"#data-transfer","title":"Data Transfer","text":"<p>Data Transfer is used for exporting data from a selected catalog to cloud storage and HDFS, including Google Cloud Storage, Amazon S3, and Azure Data Lake Storage.</p>"},{"location":"adminpanel/","title":"Admin Panel","text":"<p>Admin Panel is a feature that allows users to back up and restore the project settings, and also it is possible to create a git configuration so the changes made can be deployed there. On the right corner at the top there is an icon, when clicked it shows the Admin Panel.</p> <p></p> <p>Admin Panel has two tabs, <code>Backup</code> and <code>Git Config</code>.</p>"},{"location":"adminpanel/#example","title":"Example","text":""},{"location":"adminpanel/#create-a-backup-and-restore","title":"Create a backup and restore","text":"<p>To back up the project settings, the user must go on Admin Panel and choose <code>Backup</code> tab and click the <code>Backup &amp; Download</code> button. </p> <p></p> <p>This will create a zip file with all the settings which later on can be restored when needed by clicking on the <code>Restore</code> button and upload the same zip file.</p>"},{"location":"adminpanel/#add-a-git-configuration","title":"Add a git configuration","text":"<p>Git configuration is needed if the user wants to deploy the collection changes made in the project. In order to use the git feature the user must create one by choosing on <code>Git Config</code> tab and then click on <code>Add Git Config</code> button.</p> <p></p> <p>In <code>Git Configuration</code> the user must choose a type of the git such as GitHub, Gitlab, or Bitbucket, fill the name that should be unique, add the url, insert the branch name and provide the authentication key. </p> <p></p> <p></p>"},{"location":"adminpanel/#edit-a-git-configuration","title":"Edit a git configuration","text":"<p>Now that one git is configured, on Git Config tab the user can see a list of the git configurations and a button <code>Add Git Config</code> that redirects to the Git Configuration screen for creating a new configuration. The configuration can be edited and deleted.</p> <p></p> <p>When the user clicks on the pen icon next to the configuration from the list they can edit the type, name, url, branch name or token, and save the changes.</p> <p></p>"},{"location":"adminpanel/#delete-a-configuration","title":"Delete a configuration","text":"<p>The deletion of a configuration is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a pop-up appears with a question if the user is sure about deleting the configuration, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"alerts/","title":"Alerts","text":"<p>Alerts category allows users to set up alerts for their catalogs. Setting up alerts in Adaptive Scale is straightforward by filling some general info such as name, description, then choose a catalog, an input rule and out rule.</p>"},{"location":"alerts/#example","title":"Example","text":""},{"location":"alerts/#create-an-alert","title":"Create an alert","text":"<p>To create an alert, the user simply needs to click on the <code>Alerts</code> tab from the menu on the left. </p> <p>If there are no alerts configured, the user clicks on <code>New Alert</code> button to add a new alert.</p> <p></p> <p>After the button is clicked, the <code>Alerts</code> screen is opened. The user must fill the general information such as Alerts name, description and choose the catalog for which it needs to create the alert.  The next step is to choose an input rule between <code>Size in bytes</code>, <code>Total records</code> and <code>SQL checks</code>, and then choose an execution rule between <code>Run SQL</code>, <code>API Call</code> and <code>Send Email</code>.</p> <p></p>"},{"location":"alerts/#input-rules","title":"Input rules","text":"<p><code>Size in bytes</code> - allows users to choose a table from the selected catalog and check a value with an operator.</p> <p></p> <p><code>Total records</code> - allows users to choose a table from the selected catalog and check the total records by choosing an operator and give a value for that table.</p> <p></p> <p><code>SQL Check</code> - The SQL input rule will be fulfilled if the query returns any row.</p> <p></p>"},{"location":"alerts/#execution-rules","title":"Execution rules","text":"<p>If the input rule is fulfilled then the execution rule gets executed.</p> <p><code>Run SQL</code> - on the dropdown choose a data source, and the SQL query will be executed on that data source.</p> <p></p> <p><code>Api Call</code> - the user is able to make a GET and a POST api call, by giving the uri, and fill the header, value and the body if needed.</p> <p></p> <p><code>Send Email</code> - this allows users to send an email.</p> <p></p> <p>After clicking <code>Save</code> button all the configurations are saved and the alert is ready.</p>"},{"location":"alerts/#edit-an-alert","title":"Edit an alert","text":"<p>Now that an alert is configured, on Alerts tab the user can see a list of the alerts and a button <code>Add Alert</code> that redirects to the Alerts screen for creating a new one. The alert can be edited, deleted, activated/deactivated and searched.</p> <p></p> <p>When the user clicks on the pen icon next to the alert from the list they can edit the name, description, the catalog for the alert, also change the input and execution rules.</p> <p></p>"},{"location":"alerts/#delete-an-alert","title":"Delete an alert","text":"<p>The deletion of an alert is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a popup appears with a question if the user is sure about deleting the collection, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"alerts/#search-for-an-alert","title":"Search for an alert","text":"<p>The user can search for an alert from the list of alerts. The alert that we created as an example is named <code>Alert Demo</code>, so if the user searches for <code>test</code> there will be no results.</p> <p></p>"},{"location":"alerts/#activatedeactivate-alert","title":"Activate/Deactivate alert","text":"<p>When the user creates an alert, by default it is set in <code>Active</code> alerts list. This means that the next run of the catalog will trigger the alert, and it will be visible at the bell icon on the top.</p> <p></p> <p></p> <p>If the user clicks on the alert is able to see the alerts detailed report, the history and status of the run.</p> <p></p> <p>All this happens if the toggle is on Active, if the toggle is set to Deactivate than the user won't get an alert when it runs the catalog. The grey color indicates that the alert is deactivated.</p> <p></p>"},{"location":"catalogs/","title":"Catalogs","text":"<p>In Rosetta , the Catalog feature allows users to create catalogs based on the metadata of the data source. Catalog generation supports tagging and labeling functionalities. To create a new catalog, users must provide a unique name to prevent any potential issues and select a data source from existing sources or newly created ones. Additionally, there's a lazy loading aspect that fetches total record counts within the data source, which automatically runs every time a scheduled catalog is triggered.</p> <p>The Catalog category encompasses two main features: tagging and labeling, and scheduling. Users can schedule catalogs to run at specific time intervals. Once users have made these configurations, all changes are saved, and the new catalog is displayed in a list format. This list includes various statistics calculated during the setup process, such as the number of crawled tables and columns, the last run time, and options to edit, delete, or run the catalog.</p> <p>When the catalog is run, Rosetta  retrieves all the information from the associated data source and applies the catalog rules as defined by the user.</p>"},{"location":"catalogs/#example","title":"Example","text":""},{"location":"catalogs/#create-a-catalog","title":"Create a catalog","text":"<p>To create a data catalog, the user must click on the Catalogs tab from the menu on the left.</p> <p>If there are no catalogs configured, the user can click on <code>New Catalog</code> button to add a new catalog.</p> <p></p>"},{"location":"catalogs/#select-a-connection","title":"Select a connection","text":"<p>Same as in the <code>Data source</code> category, in <code>Catalogs</code> the user must fill the catalog name that should be unique, choose a data source connection, and all tables are filled automatically. There is an option to select all the table fields by choosing <code>All</code> or select as many as the user wants by choosing <code>Selected tables</code>.</p> <p></p> <p></p> <p>In the tables section, tables are represented as a tree view, where the user can see the table name and how many columns it has. When one of the tables or columns is selected the <code>Details</code> section is visible with all the detail of the selected table or column, such as type, number of records, number of fields and so on.</p> <p></p> <p>That table then can be expanded and the user can see all the columns together.</p> <p></p> <p>In the upper right corner, there are some buttons that have different functionalities. The difference between the ones at the top and the others inside the <code>Details</code> section is that those at the top are for the catalog schema, and those inside the <code>Details</code> section are applied only for the table or column selected.</p> <p><code>Extract</code> - JSON schema of the table or the data source. </p> <p><code>AVRO</code> - AVRO schema of the table or data source. </p> <p><code>DDL</code>- Data definition language DDL of the table or data source </p> <p>All of these schema types and DDLs can be copied and used in advance by clicking on their <code>Copy</code> button.</p> <p>There is also a <code>Prievew</code> button that when clicked shows a sample of the data.</p> <p></p>"},{"location":"catalogs/#tag-a-column","title":"Tag a column","text":"<p>In the metadata section, at the bottom right corner, the user is able to tag a table or a column. To add a tag, the user should click on the table or on the column, add a tag name on the input field next to <code>Tag</code>, and press <code>Enter</code>.</p> <p></p> <p>If the column is tagged, is has an icon on the column with the tag name on it.</p> <p></p>"},{"location":"catalogs/#label-a-column","title":"Label a column","text":"<p>Same as for tagging, labeling is one other feature in the metadata section. To add a label, the user should click on the table or on the column, add a key and value for labeling, and click on the <code>+</code> button. If the user wants to delete the label should click on the <code>-</code> button.</p> <p></p> <p>If the column is labeled, it has an icon on the column with the label on it.</p> <p></p> <p>Now that the user has finished setting a catalog it can choose whether it wants to set a schedule on the next tab or just <code>Save and Run</code> the catalog.</p>"},{"location":"catalogs/#schedule-a-run","title":"Schedule a run","text":"<p>If the user wants to add a scheduler, it should enable the add scheduler toggle, if not, it should click on <code>Save and Run</code> button.</p> <p></p> <p>Once the toggle is enabled, the user can set a scheduler to run the catalog. First should be set a start date and an end date. After that, the user can choose if the scheduler can run monthly, weekly, daily, hourly, or in minutes and in what hour.</p> <p></p> <p>To save the process the user must click on <code>Save and Run</code> button and the catalog will be created.</p>"},{"location":"catalogs/#upload-a-catalog","title":"Upload a catalog","text":"<p>To upload a catalog you need to upload the configured json on the upload catalog button. For preparing an external catalog json you need to define a few elements in the json. The mandatory fields are:</p> <ul> <li><code>name</code></li> <li><code>datasourceName</code></li> <li><code>dataSourceId</code></li> <li><code>includeAllSchemas</code></li> <li><code>schema</code></li> <li><code>schedule</code></li> </ul> <p>You can also upload an external catalog directly from API by sending the json file as a body in this endpoint <code>api/catalog/third-party</code> </p> <p>Here is an example of an external catalog:</p> <pre><code>{\n  \"dataSourceDescription\": \"test-csv\",\n  \"dataSourceId\": \"3e0fe597-5cc7-4293-a887-49a5ae588961\",\n  \"dataSourceName\": \"test-csv\",\n  \"id\": \"2c8e4027-8537-4017-bd8e-93f1e84998fa\",\n  \"includeAllSchemas\": false,\n  \"lastRun\": \"\",\n  \"lastRunId\": \"\",\n  \"name\": \"csv-catalog\",\n  \"schema\": {\n    \"name\": \"titanic.csv\",\n    \"type\": \"record\",\n    \"fields\": [\n      {\n        \"name\": \"PassengerId\",\n        \"type\": \"int\",\n        \"labels\": [],\n        \"properties\": {\n          \"description\": \"\"\n        },\n        \"sourceType\": \"int\",\n        \"tagRules\": {},\n        \"tags\": []\n      }\n    ],\n    \"labels\": [],\n    \"properties\": {\n      \"createdDate\": \"2022-10-26T11:29:09.274+02:00\",\n      \"description\": \"\"\n    },\n    \"tagRules\": {},\n    \"tags\": []\n  }\n}\n</code></pre>"},{"location":"catalogs/#edit-a-catalog","title":"Edit a catalog","text":"<p>Now that one catalog is configured, on the Catalog tab the user can see a list of the catalogs and a button <code>Add Catalog</code> that redirects to the Catalogs screen for creating a new catalog. The catalog can be edited, deleted, ran and searched.</p> <p></p> <p>When the user clicks on the pen icon next to the catalog from the list they can edit the name, tags, labels, or scheduler, and save the changes.</p> <p></p>"},{"location":"catalogs/#delete-a-catalog","title":"Delete a catalog","text":"<p>The deletion of a catalog is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a pop-up appears with a question if the user is sure about deleting the catalog, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"catalogs/#run-a-catalog","title":"Run a catalog","text":"<p>To run a catalog from the catalog list the user must click on the play icon. </p> <p>With that click, the changes made on the catalog are saved and can be viewed in the schema evolution, and data lineage.</p> <p></p>"},{"location":"catalogs/#run-history","title":"Run History","text":"<p>Run History tab lists all the runs of the catalog.</p> <p></p>"},{"location":"catalogs/#search-for-a-catalog","title":"Search for a catalog","text":"<p>The user can search for a catalog from the list of catalogs. The catalog that we created as an example is named <code>catalog data source</code>, so if the user searches for <code>test</code> there will be no results.</p> <p></p>"},{"location":"collections/","title":"Collections","text":"<p>Collection is the category where the user can create a collection based on the existing data sources, or can create a new one from scratch. The data of the data source can be explored on table designer, it can change or also create completely new ones. The collection can be linked to a git repo. </p>"},{"location":"collections/#example","title":"Example","text":""},{"location":"collections/#create-a-collection","title":"Create a collection","text":"<p>To create a collection, the user must click on Collection tab from the menu on the left.</p> <p>If there are no collections configured, the user can click on <code>Add Collection</code> button to add a new collection.</p> <p></p> <p>When <code>Collection</code> screen is opened, the user must fill a collection name that should be unique, choose a data source connection if it wants to modify an existing one or choose a DMBS type to create a new one, and the tables are filled automatically.</p> <p></p> <p></p> <p>In the tables section, tables are represented as a tree view, where the user can see the table and columns names. A new table can be created as well. When one of the tables is selected the <code>Table Designer</code> section is visible with all the detail of the selected table such as the columns it contains, their types, the primary key and nullable columns. These details can be changed or deleted by the user. Also, it is possible to add new columns and change the schema. </p> <p></p>"},{"location":"collections/#add-a-column","title":"Add a column","text":"<p>If the user wants to restructure the table it can add a column to the desired table by clicking on the table first and then click on <code>+</code> on the Table designer.</p> <p></p> <p>For a new column, the user should provide a name and a type, a size and configure a feature such as nullable or primary key.</p> <p></p> <p></p>"},{"location":"collections/#add-a-test-to-a-column","title":"Add a test to a column","text":"<p>More to add to a column will be a test with an operator. To add a test the user must hover over the column and <code>Add Test</code> button will be shown. By clicking on this button a new test will be added, so the user can choose an operator and a value to be compared to.</p> <p></p> <p></p> <p></p> <p>To save the changes the user must click on <code>Save</code> button. This will also be changed on <code>DMBL Editor</code>'s yaml file.</p> <p></p>"},{"location":"collections/#add-a-table","title":"Add a table","text":"<p>Another feature of Collections is the possibility to add a new table to the schema by clicking on <code>Add Table</code> button. On <code>Table Designer</code> the user should give a unique name to the table and continue adding columns as described above by clicking on <code>Add Column</code> button.</p> <p></p> <p>After the user is ready with setting a collection can save it with <code>Save Collection</code> button.</p>"},{"location":"collections/#edit-a-collection","title":"Edit a collection","text":"<p>Now that a collection is configured, on Collection tab the user can see a list of the collections and a button <code>Add Collection</code> that redirects to the Collections screen for creating a new one. The collection can be edited, deleted, deployed and searched.</p> <p></p> <p>When the user clicks on the pen icon next to the collection from the list they can edit the name, description, add a git repo, add or delete a table, add or delete a column and save the changes.</p> <p></p>"},{"location":"collections/#delete-a-collection","title":"Delete a collection","text":"<p>The deletion of a collection is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a popup appears with a question if the user is sure about deleting the collection, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"collections/#deploy-a-collection","title":"Deploy a collection","text":"<p>The collection that we created or updated can be deployed on a target DB. This is done by clicking on the <code>Deploy</code> button from the list.</p> <p></p> <p>A new screen for Deployment is opened where the new schema is shown on the left of the screen, a collection that can be edited and a target DB which represents the location where the collection will be deployed. The user can choose from the list of data sources where to deploy the new collection.</p> <p></p> <p></p> <p>The last step after choosing a target DB will be to click on <code>Deploy</code> button so the deployment can start, and a status of the deployment with details will be shown.</p> <p></p> <p>The actions for the deployment include download and delete.</p>"},{"location":"dashboard/","title":"Dashboard","text":"<p>The Rosetta dashboard provides users with an overview of general statistics, including the number of data sources crawled, as well as the count of tables and tags available. Users can conveniently add a data source, explore catalogs, or search for metadata tags directly from the dashboard. Moreover, users have the option to download the Rosetta driver directly from the dashboard. To facilitate users in utilizing the product effectively, comprehensive resources such as User Guides, Tutorials, and Documentation are available.</p>"},{"location":"dashboard/#example","title":"Example","text":"<p>When the user opens Rosetta, the first thing they encounter is the dashboard. As depicted in the image below, there are currently no data sources, catalogs, or tags created yet in Rosetta.</p> <p></p> <p>To modify the view of the dashboard, as previously mentioned, users can either add data sources or catalogs directly from the dashboard or add them through the menu tabs on the left.</p> <p>Users can click on the <code>Data Source</code> statistic button to create a new data source, which will redirect them to the Data source screen.</p> <p>More on  Data sources ...</p> <p></p> <p>Similarly, users can click on the <code>Catalogs</code> statistic button to create a new catalog and browse the data, redirecting them to the Catalogs screen.</p> <p>More on  Catalogs ...</p> <p></p> <p>If users have already added a data source and a catalog, they can proceed to tag a table or column from the Business Tags category. Clicking on the <code>Tags</code> statistic button directs them to the tagging process, taking them to the Business Tags screen.</p> <p>More on  Business Tags...</p> <p></p> <p>To return to the dashboard, users can simply click on the Rosetta logo.</p>"},{"location":"datasources/","title":"Data Sources","text":"<p>Data Sources category is where users establish and create connections to various data sources. Users have the flexibility to choose from fifteen options, including JDBC drivers, files, FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (Amazon S3), Microsoft Azure, HDFS, DB2, Oracle, Snowflake, BigQuery, MsSQL, PostgreSQL, MySQL, and Kinetica.</p> <p>Supported database servers encompass MySQL, MS SQL Server, Postgres, Oracle, BigQuery, and Generic drivers. To set up a new data source connection, users input details such as the connection name, description, server host and port, database name, username, and password. Additionally, users define a driver class name, upload a JDBC driver if necessary, provide a token, and more.</p> <p>Once all necessary fields are filled, the information is saved, and the newly created data source connection is displayed in a list format within the Data Source Connections section. Users can then manage these connections with options to edit or delete them.</p>"},{"location":"datasources/#example","title":"Example","text":""},{"location":"datasources/#create-a-data-source","title":"Create a data source","text":"<p>To create a data source, the user must click on the Data Source tab from the menu on the left.</p> <p>If there are no data sources configured, the user clicks on <code>Add Data Source</code> button to add a new data source.</p> <p></p> <p>When <code>Add Data Source</code> button is clicked, a new screen with fifteen options to choose from for data source connection is shown.</p> <p></p> <p></p>"},{"location":"datasources/#jdbc-connection","title":"JDBC connection","text":"<p>If the user chooses JDBC connection type, must fill in some fields such as a connection name that should be unique, a description, hostname, and port based on the DB server chosen from the dropdown list, and configured username and password for the data source, the JDBC driver should be uploaded and JDBC connection string should be given in a format <code>jdbc:sqlserver://{host}[:{port}][databasename={database}]</code>.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#file-connection","title":"File connection","text":"<p>If the user chooses file connection type the user needs to fill in a name, choose a type of the file, CSV, Avro, or parquet, and fill in the URI of the file.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#ftp-connection","title":"FTP connection","text":"<p>If the user chooses FTP connection type it needs to fill in a name, choose a type, a directory, CSV, Avro or parquet, and fill in the URI of the file starting with <code>file://</code>. In order to explore what files are inside the directory by clicking on <code>Explore</code> the user must specify the URI. If it clicks on one of the files of that directory, the URI will be rewritten automatically.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use. </p> <p></p>"},{"location":"datasources/#google-bucket-connection","title":"Google Bucket connection","text":"<p>If the user chooses Google Bucket connection type the user needs to fill in a name, choose a type, a directory, CSV, Avro, or parquet, write the service account key, and fill in the URI of the file starting with <code>gs://</code>. The user can explore what files are inside the bucket by clicking on <code>Explore</code>, even without filling the URI . If it clicks on one of the files or directories of that bucket, the URI will be filled or rewritten automatically.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p> <p></p>"},{"location":"datasources/#s3-amazon-connection","title":"S3 Amazon connection","text":"<p>If the user chooses S3 Amazon connection type the user needs to fill in a name, choose a schema type of the file, CSV, Avro, or parquet, provide access key ID and secret access key, and fill in the URI of the file starting with <code>s3a://</code>. The user can explore what files are inside the bucket by clicking on <code>Explore</code>, even without filling in the URI. If it clicks on one of the files or directories of that bucket, the URI will be filled or rewritten automatically.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#microsoft-azure-connection","title":"Microsoft Azure connection","text":"<p>If the user chooses Microsoft Azure connection type the user needs to fill in a name, choose a schema type of the file, CSV, Avro, or parquet, provide the Account name, Container name, and SAS token, and fill in the URI of the file starting with <code>wasbs://</code>. The user can explore what files are inside the bucket by clicking on <code>Explore</code>, even without filling in the URI. If it clicks on one of the files or directories of that bucket, the URI will be filled or rewritten automatically.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#hdfs-connection","title":"HDFS connection","text":"<p>If the user chooses HDFS connection type it needs to fill in a name, choose a type, a directory, CSV, Avro, or parquet, and fill in the URI of the file starting with <code>file://</code>. In order to explore what files are inside the directory by clicking on <code>Explore</code> the user must specify the URI. If it clicks on one of the files in that directory, the URI will be reswritten automatically.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> that will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p> <p></p>"},{"location":"datasources/#db2-connection","title":"DB2 connection","text":"<p>If the user chooses DB2 connection type, must fill in some fields such as a connection name that should be unique, a description, hostname, and port, configured username and password for the data source, the driver should be uploaded and the JDBC connection string should be given in a format <code>jdbc:db2://${host}:${port}</code>.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> that will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#oracle-connection","title":"Oracle connection","text":"<p>If the user chooses Oracle connection type, must fill in some fields such as a connection name that should be unique, a description, hostname, and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format <code>jdbc:oracle:thin:@//${host}:${port}</code>.</p> <p>When the user fills in all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#snowflake-connection","title":"Snowflake connection","text":"<p>If the user chooses Snowflake connection type, must fill in some fields such as the connection name that should be unique, a description, hostname, and port, configured username and password for the data source, the driver should be uploaded and JDBC connection string should be given in a format  <code>jdbc:snowflake://${host}:${port}/db=dbname</code>.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#bigquery-connection","title":"BigQuery connection","text":"<p>If the user chooses the BigQuery connection type, must fill in some fields such as the connection name that should be unique, a description, the database name, the driver that should be uploaded and the JDBC connection string should be given in a format  <code>jdbc:bigquery://{host}[:{port}][;databaseName={database}]</code>.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> that will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#microsoft-sql-server-connection","title":"Microsoft SQL Server connection","text":"<p>If the user chooses Microsoft SQL Server connection type, must fill in some fields such as a connection name that should be unique, a description, hostname, and port, configured username and password for the data source, the driver should be uploaded and the connection string should be given in a format <code>jdbc:sqlserver://{host}[:{port}][databasename={database}]</code>.</p> <p>When the user fills in all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#postgresql-connection","title":"PostgreSQL connection","text":"<p>If the user chooses PostgreSQL connection type, must fill in some fields such as the connection name that should be unique, a description, hostname, and port, configured username and password for the data source, the driver should be uploaded and the connection string should be given in a format <code>jdbc:postgresql://${host}:${port}</code>.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#mysql-connection","title":"MySQL connection","text":"<p>If the user chooses MySQL connection type, must fill in some fields such as a connection name that should be unique, a description, hostname, and port, configured username and password for the data source, the driver should be uploaded and the connection string should be given in a format <code>jdbc:mysql://${host}:${port}</code>.</p> <p>When the user fills all the fields the connection can be tested by clicking the button <code>Test</code> that will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#kinetica-connection","title":"Kinetica connection","text":"<p>If the user chooses Kinetica connection type, must fill in some fields such as a connection name that should be unique, a description, hostname and port, a database name, configured username and password for the data source, attributes such as key-value pairs that can be added, the driver should be uploaded and connection string should be given in a format <code>jdbc:kinetica:URL=http://${host}:${port}</code>.</p> <p>When the user fills in all the fields the connection can be tested by clicking the button <code>Test</code> which will turn green if the connection is correct, and turn red if the connection is wrong.</p> <p>After clicking <code>Save</code> button all the input is saved and the data source is ready for use.</p> <p></p>"},{"location":"datasources/#edit-a-data-source","title":"Edit a data source","text":"<p>Now that one data source is configured, on the Data Source tab the user can see a list of the data sources and a button <code>Add Data Source</code> that redirects to the Data source screen for creating a new data source. The data source can be edited, deleted and searched.</p> <p></p> <p>When the user clicks on the pen icon next to the data sources from the list they can edit some of the fields and save the changes.</p> <p></p>"},{"location":"datasources/#delete-a-data-source","title":"Delete a data source","text":"<p>The deletion of a data source is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a pop-up appears with a question if the user is sure about deleting the data source, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"datasources/#search-for-a-data-source","title":"Search for a data source","text":"<p>The user can search for a data source from the list of data sources. The data source that we created as an example is named <code>data source demo</code>, so if the user searches for <code>test</code> there will be no results.</p> <p></p>"},{"location":"datatransfer/","title":"Data Transfer","text":"<p>Data transfer is the category where users can initiate the transfer of data from a created catalog to a Google Storage Bucket or Amazon Simple Storage Service (Amazon S3) data source.</p>"},{"location":"datatransfer/#example","title":"Example","text":""},{"location":"datatransfer/#start-a-data-transfer","title":"Start a Data transfer","text":"<p>To start a data transfer, users must select a catalog from which they want to transfer data to a chosen target, which can be either a GCS or S3 bucket. Additionally, users need to specify the path where this transfer will be located.</p> <p></p> <p></p> <p>This functionality streamlines the process of moving data between catalogs and external storage services, facilitating efficient data management within the Rosetta platform.</p>"},{"location":"installation/","title":"Installation","text":"<p>The quickest way to deploy Rosetta is to utilize the latest Docker image. This Docker image contains all the necessary dependencies required to run Rosetta.</p> <p>To initiate the Rosetta Docker container, simply execute the following Docker command:</p> <pre><code>docker run -p 8080:8080 -e ACCEPT_EULA=Y adaptivescale/rosetta-ce:latest\n</code></pre> <p>This command will retrieve the Docker image with the most recent published version of Rosetta.</p> <p>The <code>ACCEPT_EULA</code> env variable is mandatory for confirming your acceptance of the End-User Licensing Agreement.</p> <p>Once the Docker image is deployed, Rosetta can be accessed via a browser using the following link: http://localhost:8080/</p> <p>One of the dependencies of Rosetta is ElasticSearch, which is currently bundled inside. You can configure a container to connect to an external ElasticSearch instance by setting the following environment variable: <code>ADAPTIVESCALE_ELASTIC_HOST</code>. An example of how to set it is below:</p> <pre><code>docker run -p 8080:8080 -e ADAPTIVESCALE_ELASTIC_HOST=http://localhost:9200 adaptivescale/rosetta-ce:latest\n</code></pre> <p>Rosetta comes with a default username set as <code>admin</code> and a password set as <code>admin</code>. However, you can change these credentials by providing the following environment variables: <code>PROEDMS_USER</code> and <code>PROEDMS_PASSWORD</code>. Here's an example of how to use this:</p> <pre><code>docker run -p 8080:8080 -e PROEDMS_USER=testuser -e PROEDMS_PASSWORD=testpassword adaptivescale/rosetta-ce:latest\n</code></pre>"},{"location":"schedules/","title":"Schedules","text":"<p>Schedules category provides users with a centralized view of all scheduled catalogs. This section lists important details such as the catalog name, the timestamp of the last run, the schedule for the next run, and options to activate or deactivate a schedule, as well as the ability to delete it altogether.</p> <p>By offering this comprehensive overview, users can easily manage and monitor their scheduled catalog operations, ensuring efficient and timely execution of data processing tasks.</p>"},{"location":"schedules/#example","title":"Example","text":""},{"location":"schedules/#create-a-schedule","title":"Create a schedule","text":"<p>To create a schedule, the user must click on the Schedules tab from the menu on the left or whenever the user creates a Catalog.</p> <p>If there are no schedules created, the user clicks on <code>Add Schedule</code> button to add a new schedule, and select a catalog from the list which redirects the user to the <code>Scheduke</code> tab of the chosen catalog to set a schedule.</p> <p></p> <p>Once the user schedules a run for a catalog, in the Catalogs tab, it is listed in the Schedules category. The list of schedules has information like the catalog name, schedule, last run and next run. It also has the possibility to delete and activate or deactivate a schedule.</p> <p></p>"},{"location":"schedules/#activate-schedule","title":"Activate schedule","text":"<p>When the user schedules a catalog, by default it is listed in <code>Active</code> scheduled list. This means that the next run of the catalog will happen on the date scheduled in the list.</p> <p>If the user wants to stop the next run, it can deactivate the catalog run by clicking on the eye icon.</p> <p></p>"},{"location":"schedules/#deactivate-schedule","title":"Deactivate schedule","text":"<p>When the user deactivates a catalog schedule run, by clicking on deactivate button, that catalog then will be listed in <code>Inactive</code> scheduled list. This means that the next run of the catalog will not happen on the date scheduled in the list until it is activated again.</p> <p>If the user wants to activate the catalog run it is done by clicking on the eye icon.</p> <p></p>"},{"location":"schedules/#delete-a-schedule","title":"Delete a schedule","text":"<p>The deletion of a scheduled catalog run is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a pop-up appears with a question if the user is sure about deleting the schedule, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"schemaevolution/","title":"Schema Evolution","text":"<p>In Rosetta, the Schema Evolution category empowers users to track and visualize changes made to catalogs over time. This feature provides a clear view of data lineage, allowing users to observe alterations made to columns and schemas.</p> <p>When users select a specific date, any changes made to the catalog since that time are displayed. For enhanced clarity, modified columns are highlighted in orange, while deleted columns are highlighted in red. This intuitive color-coded system enables users to quickly identify and understand the evolution of their data schemas.</p> <p>This functionality is invaluable for maintaining data integrity and ensuring transparency in data management processes.</p>"},{"location":"schemaevolution/#example","title":"Example","text":"<p>To view the catalog changes over time, the user must click on <code>Schema Evolution</code> category tab and choose a catalog that wants to view the data lineage.</p> <p></p> <p>When the user clicks on the catalog, a new graphical view of all the runs is shown. The graph points represent the data lineage in time when that catalog was run.</p> <p></p> <p></p> <p>With this feature, the user is able to compare the catalog changes during specific times.</p> <p>For example, in the picture below, during the first run, Generation 1 there were 30 fields and in the second run there are only 22 fields, where 8 fields are deleted and are colored with red.</p> <p></p> <p>If the user hovers on the point of change in the graph there will be a detailed view of what was changed during the runs.</p> <p></p> <p>Now if the user goes to the Catalogs, adds some fields, and runs the catalog, it can see the new run in the data lineage, Generation 2. From two runs that were in the previous example, now there are three v1, v2, and v3.</p> <p>The new columns that are added are colored with green and the number of fields from 22 became 23.</p> <p></p> <p></p> <p>Lastly, if the user wants to make a change in the current schema, add a tag or remove one, it needs to return back to Catalogs, make the changes and run the catalog again. From three runs that were in the previous example, now there will be four.</p> <p>The columns that are modified are colored orange and the user can see the tag and label icon on the columns.</p> <p></p> <p></p> <p>All these changes in one are captured in the picture below. Column <code>actor</code> is modified and colored with orange, column <code>payment</code> is deleted, has removed the label and colored with red, and column <code>actor_salary</code> is added to the schema and colored with green.</p> <p></p> <p>To view more details the user can expand the list of changes and can see them described.</p> <p></p>"},{"location":"search/","title":"Metadata Search","text":"<p>Search category provides users with a powerful tool for searching and retrieving data based on various criteria such as tags, field names, table names, and data source names. Users can utilize a range of supported filters including string and wildcard searches.</p> <p>Supported filters include:</p> <ul> <li><code>*(string)*</code></li> <li><code>wild card: *(string)*</code></li> <li><code>tag: &lt;tag name (string)&gt;</code></li> <li><code>key: &lt;key name (string)&gt;</code></li> <li><code>val: &lt;(string)&gt;</code></li> </ul> <p>For example, if a user searches for a specific tag name, Rosetta  queries the ElasticSearch database to retrieve matches and provides the user with the results. Additionally, users can explore data lineage directly from the search section by clicking on specific columns.</p>"},{"location":"search/#example","title":"Example","text":"<p>In the Metadata Search tab, users can search by tags, field names, table names, data source names, and more.</p> <p></p> <p>Let's perform a search for each of these criteria.</p> <p>From earlier examples, we can recall that we tagged a column with a tag named <code>pii</code>, so if we search for <code>pii</code>, we will have a result.</p> <p></p> <p>Now that the user has searched for <code>pii</code>, the results are displayed in a list format, accompanied by a dashboard showing statistics such as the number of data sources, records, fields, tags, and labels crawled from that search.</p> <p></p> <p>This functionality allows users to efficiently explore and analyze data within Rosetta , enabling informed decision-making and data-driven insights.</p>"},{"location":"security/","title":"Security","text":"<p>When it comes to security Rosetta is divided in two sections:</p> <ul> <li>Secure store - for storing credentials and other secrutiy related info used during operations like accessing data source.</li> <li>Authentication (LDAP) - for allowing/restricting acces to the platform.</li> </ul>"},{"location":"security/#secure-store","title":"Secure Store","text":"<p>To secure credentials platform utilizes Java KeyStore in JKS format to create a secure store file. Location of this file and the password used for it's encryption can be configured in <code>application.properties</code> file. Note: these values need to be modified before any data source is added - otherwise all data source passwords for existing data sources need to be re-entered through UI-&gt;Data Source-&gt;Edit form.</p> Property Value Required Description proedms.secure.store.file keystore.jks Yes Location of the file where to store keys proedms.secure.store.type JCEKS Yes Type of the keystore. Currently only <code>JCEKS</code> is supported"},{"location":"security/#authentication-ldap","title":"Authentication (LDAP)","text":"<p>Authentication in Rosetta is optional and can be enabled/disabled from <code>application.properties</code> file. Rosetta uses LDAP for authentication and it also provides the ability to bring up local server based on provided LDIFF file. Once the user credentials are verified JWT token is created which is valid for 24 hours and all subsequent interactions have this token provided in <code>Authorization</code> header property.</p> <p>There are few properties that are required to be set in <code>application.properties</code> in order to enable authentication.</p> Property Value Required Description proedms.security.authentication.enabled true Y This property enables the authenticaiton mechanism in general. Default: false proedms.jwt.auth.type SECRET Y Indicates the type of secret JWT will use to has/ecnrypt tokens. Can be set to <code>SECRET</code> for using string type of key. Can be set to <code>KEY</code> for using private key file. proedms.jwt.secret [String/Path] Y Depending on how the property <code>proedms.jwt.auth.type</code> this can be either a string type of secret or location to private key file. ldap.user.dn.patterns ex:uid={0},ou=people Y User DN Pattern - A DN pattern that can used to directly login users to the LDAP database. This pattern is used for creating a DN string for \"direct\" user authentication, where the pattern is relative to the base DN in the ldapUrl. Note: uid needs to have {0} as value as it will be matched against username provided during login. ldap.group.search.base ex: <code>ou=groups</code> Y Subtree to search for username ldap.url ex: <code>ldap://localhost:8389/dc=AdaptiveScale,dc=com</code> Y LDAP url to connect to - can be set as localhost if Rosetta embeded LDAP is to be used. ldap.password.attribute userPassword Y Name of the attribute for password in LDAP. Default:<code>userPassword</code>. <p>Rosetta also provided embeded LDAP server that can be activated by providing additional properties to properties file. This is useful for the cases where users don't have their own LDAP server or don't want to connect the platform with existing LDAP server. Keeping the configuration sa examples from abvoe and additionally setting the following configurations will allows users to start Rosetta embedded LDAP server based on provided LDIFF file which will contains LDAP data.</p> Property Value Required Description spring.ldap.embedded.ldif file:///opt/apt/auth.ldif Y Absolute path to LDIFF file spring.ldap.embedded.base-dn dc=AdaptiveScale,dc=com Y A base dn from where the server will search for users. This is dependent with LDIFF contents and is also needed in <code>ldap.url</code> property. spring.ldap.embedded.port 8389 Y Port on which embedded LDAP server will run. <p>Using embedded server requires LDIFF file which is a copy of LDAP data structure dumped to single file. The following example ldiff matches the example configurations given above and creates user with username <code>admin</code> and password <code>admin</code>. Create a file with <code>.ldif</code> extension and add the follwing as it's content.</p> <pre><code># LDAP Auth data\ndn: dc=AdaptiveScale,dc=com\nobjectclass: top\nobjectclass: domain\nobjectclass: extensibleObject\ndc: AdaptiveScale\n\n# Organizational Units\ndn: ou=groups,dc=AdaptiveScale,dc=com\nobjectclass: top\nobjectclass: organizationalUnit\nou: groups\n\ndn: ou=people,dc=AdaptiveScale,dc=com\nobjectclass: top\nobjectclass: organizationalUnit\nou: people\n\n# Accounts\ndn: uid=admin,ou=people,dc=AdaptiveScale,dc=com\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\ncn: AdaptiveScale\nsn: AdaptiveScale\nuid: admin\nuserPassword: admin\n\n# Create Group Admin\ndn: cn=admins,ou=groups,dc=AdaptiveScale,dc=com\nobjectclass: top\nobjectclass: groupOfUniqueNames\ncn: admins\nou: admin\nuniqueMember: uid=admin,ou=people,dc=AdaptiveScale,dc=com\n</code></pre> <p>Starting Rosetta with the configration set as above will land user in Login screen where user <code>admin</code> and password <code>admin</code> can be used to authenticate.</p>"},{"location":"security/#authorization-ldap","title":"Authorization (LDAP)","text":"<p>Rosetta utilizes groups/roles for ACL, both authentication and authorization is based on LDAP, this can be configured either from the <code>auth.ldif</code> file or from a LDAP server. Access is divided into the following categories:</p> Category name Category description DS Stands for data-source - handles access for resources under data-source CAT Stands for catalog - handles access for resources under catalog DEP Stands for deployment - handles access for resources under deployment COL Stands for collection - handles access for resources under collection DT Stands for data-transfer - handles access for resources under data-transfer TAG Stands for tag - handles access for resources under tag SCHED Stands for schedule - handles access for resources under schedule SEARCH Stands for search - handles access for resources under search EV Stands for schema-evolution - handles access for resources under schema-evolution ALERT Stands for alert - handles access for resources under alert SQLEXPL Stands for sql-explorer - handles access for resources under sql-explorer GIT Stands for git-configuration - handles access for resources under git-configuration <p>Besides the categories there is also operational level permissions:</p> Permission name Permission description READ This permission allows the user to view/list resource(s) WRITE This permission allows the user to modify resource(s) EXECUTE This permission allows the user to execute operations on resource(s) <p>ACL in Rosetta is achieved by creating a group name that combines the category name as a prefix and permission name as a suffix.</p> <p>Like the following example defines a group which has permission to view/list data-sources and assigns the user <code>test</code> as a member of the group which will allow user to have access to the data-sources:</p> <pre><code># GROUP DS-READ\ndn: cn=adaptive-ds-readers,ou=groups,dc=adaptivescale,dc=com\nobjectClass: top\nobjectClass: groupOfNames\ncn: adaptive-ds-read\nmember: uid=test,ou=people,dc=adaptivescale,dc=com\n</code></pre> <p>Besides the combinations that can be achieved above there is also an <code>adaptive-admin</code> group, members of which have full access to all resources, including the capability to delete resources.</p> <p>Example given below shows how an <code>adaptive-admin</code> group looks like:</p> <pre><code># GROUP ADMIN\ndn: cn=adaptive-admins,ou=groups,dc=adaptivescale,dc=com\nobjectClass: top\nobjectClass: groupOfNames\ncn: adaptive-admin\nmember: uid=admin,ou=people,dc=adaptivescale,dc=com\n</code></pre> <p>Note: Enabling and disabling permissions reflects on the UI components visibility as well.</p>"},{"location":"sqlexplorer/","title":"SQL Explorer","text":"<p>SQL Explorer is the category designed to assist users in extracting information from various data sources.</p> <p>The interface presents users with a dropdown menu to select the data source they wish to query and a SQL Explorer editor where users can input structured query language-formatted queries.</p> <p></p> <p>Key features include:</p> <ul> <li>A \"Run Query\" button that executes the query entered in the editor.</li> <li>A \"Limit\" dropdown menu that specifies the maximum number of results to display, with options including 10, 100, and 1000.</li> </ul> <p>When users select a data source from the dropdown menu, all tables and columns associated with that data source are displayed in a tree view on the left side of the screen. This visual representation facilitates users in inspecting the contents of their data source and simplifies the process of querying information.</p> <p></p>"},{"location":"sqlexplorer/#example","title":"Example","text":"<p>Since no query has been executed yet, no results are displayed. Let's demonstrate by running some queries to observe the results table.</p> <p>For this example, let's retrieve all the <code>actors</code> from the data source. Following the standard SQL syntax, we start with the \"SELECT\" command to specify the fields we want to retrieve. The \"FROM\" command indicates the source from which to extract data. Upon clicking the <code>Run Query</code> button, the result table will be generated.</p> <p></p> <p>Below the SQL editor, the results table is populated with data retrieved from the executed query. The table may contain multiple records, which are displayed across pages based on the selected limit from the dropdown menu.</p> <p>This functionality streamlines the process of querying data, enabling users to extract valuable insights from their data sources effectively.</p>"},{"location":"tags/","title":"Tags","text":"<p>Tags category allows users to create business tags for organizing and labeling data. To create a business tag, users must provide a tag name, description, and collection name. Alternatively, users can import tags from a CSV file, offering another convenient method for adding multiple tags at once.</p> <p>Once the necessary information is filled out or imported, the business tag is created and available for use within the system. These tags serve as a powerful tool for categorizing and organizing data, enhancing its usability and accessibility across various workflows.</p>"},{"location":"tags/#example","title":"Example","text":""},{"location":"tags/#create-a-business-tag","title":"Create a business tag","text":"<p>To create a business tag, the user must click on Tags tab from the menu on the left.</p> <p>If there are no tags created, the user clicks on <code>Add Business Tag</code> button to add a new tag or clicks on <code>Import From CSV</code> which is an option to import a CSV file as a new tag.</p> <p></p>"},{"location":"tags/#business-tag","title":"Business tag","text":"<p>When <code>Add Business Tag</code> button is clicked, the user must fill the tag name with a unique name, a description for that tag and a collection name.</p> <p></p> <p>A matching pattern is an act of checking a given sequence of tokens for the presence of the constituents of some pattern. In the input field of the matching pattern, we can give a name of a data source column, field, or the data source itself to tag it accordingly.  In the previous section in Data sources, we can recall that we had a column named <code>actor</code>.</p> <p>Let us use that column for an example with a matching pattern. In the matching pattern input field, we put the name of the column from the data source <code>actor*</code> which means that all tables or columns that have an actor in their name will be tagged with <code>pii</code> tag.</p> <p></p> <p>We can see the result when we run the catalog, on the Catalog tab or Schema evolution. So table <code>actor</code>, <code>actor_genre</code> and column <code>actor_id</code> are all tagged with <code>pii</code> tag.</p> <p>Business rules are used for applying rules like masking, on the data itself for all the fields that have the given matching tag.</p> <p>After clicking <code>Save</code> button all the input is saved and the business tag is created.</p>"},{"location":"tags/#import-csv","title":"Import CSV","text":"<p>Business tags can be added also by uploading CSV files. The CSV file should have the following columns: <code>Collection</code>, <code>Tag</code>, <code>Pattern</code> and <code>Description</code>, for adding multiple matching rules in <code>Pattern</code> column the user should separate them with space.</p> <p>The CSV file: </p> <p>The Business Tags table:</p> <p></p>"},{"location":"tags/#edit-a-business-tag","title":"Edit a business tag","text":"<p>Now that one business tag is created, on Tags tab the user can see a list of the tags and a buttons as <code>Add Business Tag</code> and <code>Import from CSV</code> that redirects to the Tags screen for creating a new business tag. The tag can be edited, deleted, activated or deactivated and searched.</p> <p>When the user clicks on the pen icon next to the tag from the list it can edit some of the fields and save the changes.</p> <p></p>"},{"location":"tags/#delete-a-business-tag","title":"Delete a business tag","text":"<p>The deletion of a tag is done by clicking on the bin icon.</p> <p></p> <p>When the icon is clicked, a pop up appears with a question if the user is sure about deleting the tag, and if yes the user clicks Delete and if not clicks Cancel.</p> <p></p>"},{"location":"tags/#search-for-a-business-tag","title":"Search for a business tag","text":"<p>The user can search for a tag from the list of business tags. The tag that we created as an example is named <code>tag</code>, so if the user searches for <code>test</code> there will be no results.</p> <p></p>"},{"location":"tags/#activate-a-tag","title":"Activate a tag","text":"<p>When the user creates a tag, by default it is listed in <code>Active</code> tags list.</p> <p>If the user wants to deactivate the tag it can click on the eye icon.</p> <p></p>"},{"location":"tags/#deactivate-a-tag","title":"Deactivate a tag","text":"<p>When the user deactivates a tag, by clicking on deactivate button, that tag then will be listed in <code>Inactive</code> tags list.</p> <p>If the user wants to activate the tag it is done by clicking on the eye icon.</p> <p></p>"}]}